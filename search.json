[{"title":"NBRW(SIGMETRICS 2012) 不回溯的无偏图采样","url":"/2019/12/15/NBRW-SIGMETRICS-2012-不回溯的无偏图采样/","content":"\n[Lee, Chul-Ho, Xin Xu, and Do Young Eun. \"Beyond random walk and metropolis-hastings samplers: why you should not backtrack for unbiased graph sampling.\" ACM SIGMETRICS Performance evaluation review. Vol. 40. No. 1. ACM, 2012.](http://delivery.acm.org/10.1145/2260000/2254795/p319-lee.pdf?ip=222.195.68.252&id=2254795&acc=ACTIVE%20SERVICE&key=BF85BBA5741FDC6E%2EA4F9C023AC60E700%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1576408507_9dea440cf445046ba6683f1b18b61c97)\n\n### 无偏图采样\n\n#### 无偏图采样技术\n通过爬行的无偏图采样技术，是指通过一个或多个在图上爬行的随机游走，进行的均匀节点采样，最终分析估计目标图的节点或者拓扑特性。具体来说，无偏均匀图采样即对于任意给定的一个满足均匀分布的期望函数$f$，即$E_u(f) \\stackrel{\\Delta}{=} \\sum_{i\\in N}{f(i)/n}$，开发一个基于随机游走的无偏估计器。（一个图上的随机游走，若其状态转移矩阵P满足不可约的特性，则该随机游走所表示的马尔可夫链存在唯一的一个稳态分布$\\pi$。）\n\n#### 基于随机游走的无偏图采样\n\n对任意一个函数$f$，可以定义一个基于随机游走的估计器：$\\hat{\\mu}_t(f) \\stackrel{\\Delta}{=} \\frac{1}{t} \\sum_{s=1}^t{f(X_s)}$,\n\n函数$f$关于$\\pi$的期望为$E_{\\pi}(f) \\stackrel{\\Delta}{=} \\sum_{i\\in N}{f(i)\\pi(i)}, E_{\\pi}(|f|)< \\infty$。\n\n**强大数定理（SLLN, Strong Law of Large Numbers）**保证了，当$t \\rightarrow \\infty$，$\\hat{\\mu}_t(f) \\rightarrow E_{\\pi}(f) a.s.$。\n\n#### 不同随机游走采样的评判指标\n\n一个图上的很多种随机游走可能都能达到同一个稳态分布$\\pi$，可以用来进行相同的无偏估计，如何去评判哪一种基于随机游走的采样算法更好呢？\n\n**混合时间（mixing time）**是刻画图上随机游走收敛速度的一个重要指标，适合用来比较不同的随机游走。\n\n但是混合时间（mixing time）却不适合用来比较不同的随机游走采样，因为基于随机游走的采样虽然通常是需要经历一个时期进行收敛（burn-in period），以消除初始节点的影响，但是在随机游走收敛之后，仍然需要获取很多样本从而进行无偏估计。所以，对于基于随机游走的无偏图采样来说，估计器的效率，即在保证一定精度的情况下，所需要的最少的**样本的数目**才是最重要的评判指标。\n\n#### 渐进方差\n\n定义上述基于随机游走的无偏估计器的渐进方差\n\n$$\\sigma^2(f) \\stackrel{\\Delta}{=} lim_{t \\rightarrow \\infty} t \\cdot Var(\\hat{\\mu}_t(f))$$\n\n**极限中心定理（CLT, Central Limit Theorem）**表明$\\sigma^2(f)$是$\\hat{\\mu}_t(f)$的方差，可以根据$\\sigma^2(f)$来决定，在保证一定精确度的情况下所需要的采样样本的数目。$\\sigma^2(f)$值越小，则对应的基于随机游走的无偏估计器的效率就越高，即在保证相同精确度的情况下所需要的样本数目越少。\n\n**该篇论文的目标：通过减小$\\sigma^2(f)$的值来减少基于随机游走的无偏估计器所需要的样本数目。**\n\n### 基于随机游走的采样算法\n\n#### SRW-rw\n\n在图上进行简单的随机游走（SRW, Simple Random Walk），其访问的节点序列组成一个马尔可夫链${X_t}$，其对应的状态转移矩阵为$P={P(i,j)=\\frac{1}{d_i}}$，稳态分布为$\\pi={\\pi(i)=\\frac{d_i}{2m}}$。\n\n根据随机游走获取的样本进行无偏估计，可以设置无偏估计的权重$w(i)=\\frac{u(i)}{\\pi(i)}=\\frac{2m}{nd_i}$，则无偏估计器为\n\n$$\\hat{\\mu}_t(wf) \\stackrel{\\Delta}{=} \\frac{1}{t} \\sum_{s=1}^t{w(X_s)f(X_s)}$$\n\n当$t \\rightarrow \\infty$，$\\hat{\\mu}_t(wf) \\rightarrow E_{\\pi}(wf) = E_{u}(f) a.s.$。\n\n但是，由于$n$和$m$在很多场景下是未知的，所以$w(i)=\\frac{2m}{nd_i}$无法计算，所以可以使用另一种替代的无偏估计的方法：\n\n$$\\frac{\\hat{\\mu}_t(wf)}{\\hat{\\mu}_t(w)} \\stackrel{\\Delta}{=} \\frac{\\sum_{s=1}^t{w(X_s)f(X_s)}}{\\sum_{s=1}^t{w(X_s)}}$$\n\n此时，可以设置$w(i)=\\frac{1}{d_i}$，计算结果相同。\n\n#### MHRW\n\n主要思想：调整随机游走的状态转移概率，使得其马尔可夫链对于的稳态分布$\\pi=u$。\n\n**（1）MH（Metropolis-Hastings）算法**\n\nMH算法首先设置一个**建议概率（proposal probability）$Q(i,j)$**（$Q(i,j)>0$当且仅当$Q(j,i)>0$，即只给与互为邻居的节点之间正的转移概率），然后建议状态过渡到$X_{t+1} =j$后以一定的**接受概率（acceptance probability）$A(i,j)$**接收该次转移，$A(i,j)= min(1,\\frac{\\pi(j)Q(j,i)}{\\pi(i)Q(i,j)})$。最终，MH算法真实是状态转移概率为：\n\n$$P(i,j)=min(Q(i,j),Q(j,i)\\frac{\\pi(j)}{\\pi(i)}$$\n\n**（2）MHRW采样**\n\n设置建议概率$Q(i,j)=\\frac{1}{d_i}$，则状态转移概率为$P(i,j)=min(\\frac{1}{d_i},\\frac{1}{d_j}), if (i,j) \\in E$。\n\n上述状态转移矩阵所对应的是一个时间可逆的马尔可夫链，所以$\\pi_iP_{ij}=\\pi_jP{ji}$，又因为$P_{ij}=P{ji}$，\n\n$\\Rightarrow \\pi_i=\\pi_j, if (i,j) \\in E$，\n\n$\\Rightarrow \\pi_i=\\frac{1}{n}$，\n\n$\\Rightarrow \\pi=u$。\n\n#### 存在问题 & Idea\n\n存在问题：SRW和MHRW都容易回到之前一步访问过的节点，导致短时间内产生重复样本，带来较大的渐进方差。\n\nIdea：修改随机游走的状态转移概率，避免回到前一步访问过的节点。\n\n待解决的的挑战：1）如何保证不改变原本的稳态分布；2）如何实现最小的额外开销；3）证明能减小采样算法的渐进方差。\n\n### 避免回溯的随机游走图采样\n\n#### 从可逆到不可逆\n\n若要实现不回溯的随机游走，随机游走的下一跳选择不仅取决于当前节点，还取决于上一步的节点，所以在状态空间$N$上的不回溯的随机游走，不再是一般意义上的马尔可夫链了。\n\n构建新的状态空间$\\Omega \\stackrel{\\Delta}{=} ( (i,j), i,j\\in N, s.t. P(i,j)>0 )$， 其中一个状态表示为$e_{ij}$。在状态空间$\\Omega$上构建新的马尔可夫链，对于的状态转移概率和稳态分布分别表示为$P'(e_{ij},e_{jk})$和$\\pi'(e_{ij}$。\n\n如果新的马尔可夫链满足：$\\pi'(e_{ij}=\\pi(i)P(i,j)$，且原来的马尔可夫链是时间可逆的，则$\\pi'(e_{ij})=\\pi'(e_{ji})$。\n\n且新的马尔可夫链在达到稳态分布时，停留在节点$j$的概率就等于原马尔可夫链的概率$\\pi(j)$，即：\n\n$$\\sum_{e_{ij}\\in \\Omega}{\\pi'(e_{ij})} = \\sum_{i \\in N}{\\pi(i)P(i,j)}=\\pi(j)$$\n\n我们可以利用新的马尔可夫链进行无偏采样：令$g(e_{ij})=f(j)$，则$E_{\\pi'}(g)=\\sum_{e_{ij} \\in \\Omega}{g(e_{ij})\\pi'(e_{ij})}=E_{\\pi}(f)$。\n\n**定理3（由一个可逆的马尔可夫链构建其相应的不可逆的马尔可夫链）：**\n\n*假定$X_t$是状态空间 $N$上的一个不可约且可逆的马尔可夫链，其状态转移概率和稳态分布分别为$P={P(i,j)}$和$\\pi$。构建状态空间 $\\Omega$上的一个新的马尔可夫链$Z_t$，使得其转移概率$P'={P'(e_{ij},e_{lk})}$满足下面两个条件：*\n\n$$P(j,i)P'(e_{ij},e_{jk})=P(j,k)P'(e_{kj},e_{ji})$$\n\n$$P'(e_{ij},e_{jk}) \\geq P(j,k)$$\n\n*那么$Z_t$是一个不可约且不可逆的马尔可夫链，其存在唯一的一个稳态分布$\\pi'$，且满足$\\pi'(e_{ij})=\\pi(i)P(i,j)$，并且对任意一个函数$f$，满足$\\sigma'^2(f) \\leq \\sigma^2(f)$。*\n\n#### NBRW-rw\n\n#### MHDA\n\n### 模拟实验","tags":["paper reading","random walks","theoretical analysis","graph sampling"]},{"title":"Random Walks on Graphs -- A Survey","url":"/2019/12/11/Random-Walks-on-Graphs-A-Survey/","content":"\n[Lovász L. Random walks on graphs: A survey[J]. Combinatorics, Paul erdos is eighty, 1993, 2(1): 1-46.](http://www.cs.yale.edu/publications/techreports/tr1029.pdf)\n\n这是一个关于**随机游走**的综述性的调研报告，包含与随机游走相关的很多问题，涵盖了*基础的随机游走模型*，*访问时间*，*覆盖时间*，*收敛速度*，*特征值分析*，*电气网络*以及*应用算法*等等。\n\n### 随机游走简介\n\n给定一个图和一个起始节点，随机选择当前节点的一个邻居节点并跳转，持续上述过程，产生一串随机的节点的过程称为**图上的随机游走**。\n\n图上的随机游走的过程可以看成是一个有限的马尔可夫链，无向图上的随机游走可以看成是时间可逆的马尔可夫链。\n\n实际上，现实生活中很多场景都可以看成是随机游走，比如*洗一副扑克牌*的过程，所有纸牌的排列置换可以构成一个图的节点，若通过一次洗牌可以实现两个状态的转换，则将相应的两节点相连，则持续洗牌的过程即可看成是一个图上随机游走的过程。\n\n最早的随机游走通常简单、无限的图上的随机游走来定性的分析某些特征，比如随机游走是否能回到初始节点？；之后，随机游走开始更多的面向通用的、有限的图的场景，并且更多的进行定量的分析研究，比如随机游走需要多少步能够回到起始节点？随机游走多快能到达极限分布？\n\n研究发现，随机游走与很多图理论相关，比如图的**谱理论**，**电气网络**中的电阻等等。本篇调研报告将关注**特征值**和**电气网络**。最后，本文调研了一些相关的应用场景。\n\n### 基础知识\n\n随机游走 -- 马尔可夫链 -- 状态转移矩阵\n\n当随机游走进入一个状态的分布后，状态的分布不再改变，则称其为**稳态分布**，对每个连通图，都有唯一的稳态分布：$\\pi(v)=\\frac{d(v)}{2m}$。\n\n当图为正则图（各定点度数均相同），则称该图上的随机游走对应的马尔可夫链是**对称的**。当图不是正则图，该性质由**时间可逆性**代替。对于时间可逆的马尔可夫链的任意一对节点$i,j$，都有$\\pi_i P_{ij}=\\pi_j P_{ji}$。\n\n### 主要参数\n\n#### 主要参数的定义\n\n**（1）访问时间（access time / hitting time）**，$H(i,j)$，表示从节点$i$出发访问到节点$j$的期望步数。\n\n**（2）通讯时间（commute time）**，$\\kappa(i,j)=H(i,j)+H(j,i)$，表示从节点$i$出发访问到节点$j$，之后又回到节点$i$的期望步数。\n\n**（3）覆盖时间（cover time）**，表示从一个初始节点出发，访问到图中每个节点的期望步数。若未指定初始节点，则是指最差情况下的期望步数。\n\n**（4）混合速率（mixing rate）**，表示随机游走收敛到其极限分布的速率，定义为：$\\mu=lim_{t\\to \\infty} \\sup max_{i,j}|p_{ij}^{(t)}-\\frac{d_j}{2m}|^{1/t}$\n\n#### 相关性质\n\n* 主要参数的界\n\n* 对称性和访问时间\n\n* 访问时间和覆盖时间\n\n* 单调性\n\n* 覆盖时间和通讯时间的界的应用场景\n\n### 特征值连接\n\n谱（spectral）理论\n\n* 谱和访问时间\n\n* 谱和生成函数\n\n### 电气连接 \n\n### 混合速率\n\n* 混合速率和耦合\n\n* 混合速率和特征值间隔（eigenvalue gap）\n\n* 特征值间隔和导通性\n\n* 导通性和多商品流\n\n### 随机游走采样","tags":["random walks","theoretical analysis"]},{"title":"GraphX(OSDI 2014) 分布式数据流框架上的图处理","url":"/2019/11/24/GraphX-OSDI-2014-分布式数据流框架上的图处理/","content":"\n### 图处理场景\n\n#### 分布式流处理框架\n\n通用分布式数据流框架，比如MapReduce和Spark，能支持丰富的数据流操作，但是却更适用于分析处理非结构化或表格类的数据，在处理迭代的图算法时多个阶段的复杂连接，而且没有利用到迭代图算法中的常见模式和结构，因此利用他们进行迭代图算法分析往往性能很差。\n\n#### 专用图处理系统\n\n专用的图处理系统能很好的表达迭代图算法，进行图处理时通常比通用的分布式数据流框架能好几个数量级。\n\n图系统中一般将图结构数据表示为一个属性图，每个节点和边有其相应的属性，其中属性包括元数据（例如用户配置文件和时间戳）和程序状态（例如PageRank值）。\n\n专用图系统的不足：\n* （1）现实应用中，图分析任务往往伴随着其他类型数据的分析处理，比如非结构化的数据或表格数据。此时就需要组合多种系统进行管道处理流程，此过程中不免提升了计算的复杂度，带来了不必要的数据迁移和移动。\n* （2）现在图处理系统一般为了性能优化而放弃了考虑容错。\n所以专用的图处理系统通常没有被分布式数据流框架的广泛支持。\n\n#### 局限性与希冀\n\n过去，图处理系统和分布式数据流框架是分开发展的，主要原因有：\n* （1）早期的分布式数据流框架（比如MapReduce）强调单阶段计算和在磁盘上处理，而迭代图算法需要重复且随机地访问图的子集，所以很难应用于这些计算框架。\n* （2）早期的分布式数据流框架没有公开对数据分区的细粒度控制，从而阻碍了图数据分区技术的应用。\n\n但是，新提出的内存分布式数据流框架（比如Spark和Naiad）公开了对数据的划分和数据的内存表示，已经解决了一部分上述的局限性。\n\n考虑*分布式流处理框架*相对*专用图处理系统*的优点：\n* 数据流系统中操作往往可以跨越多个集合，但图系统中操作只定义在单个的属性图上。\n\n**希冀**：结合图处理系统和分布式流数据框架的优点，使得一个系统能处理整个分析管道。\n\n#### 目标与实现\n\n所以本文的想法是开发一个**通用的分布式数据流框架之上的图处理系统**，需求：\n* （1）识别图计算的基本数据流模式；\n* （2）将图处理系统中的优化转换为数据流优化。\n\n本文在Spark的基础上构建了GraphX库，GraphX提供了图API,并在上层实现了Pregel抽象以及一些图操作。GraphX将图计算嵌入到分布式流处理框架中，将图计算的过程提取成一个特定的“join–map–group-by”的数据流模式。不同于现有的图处理系统，GraphX的API支持图数据和非结构化、表格数据的组合。并且允许将同一个物理数据同时视为图数据和集合数据，避免数据移动和复制。\n\n但是，直接将图编码为集合，然后直接使用通用的数据流操作执行迭代图计算非常低效。为了能实现与专有图系统相当的性能，GraphX进行了如下方面的优化：\n* （1）如何将图编码为集合；\n* （2）如何执行通用的数据流操作。\n\n具体的，将专用图处理系统中的有几个优化点重现应用到spark数据流操作中：\n* （1）切割点划分 -- 水平划分的集合\n* （2）活跃节点记录 -- 增量视图\n* （3）节点镜像 -- 基于路由表的多播连接\n\n真实数据集上的实验表明，GraphX能实现和专用图系统相当的性能，同时保留通用数据流框架的优点。\n\n### GraphX的编程抽象\n\n#### 集合表示属性图\n\nGraphX中使用一对*节点集*和*边集*来表示一个*属性图*。**节点集**包含节点属性，由*节点ID*唯一标志，。**边集**包含边属性，由*源节点和目的节点ID*唯一标志。\n\n通过将一个属性图表示成一对节点集和边集的形式，使得GraphX能轻易将分布式数据流框架中的其他集合与图组合。比如增加一个节点属性只需要与相应的节点属性集合连接。分析图计算的结果以及比较不同图的属性也只需要进行连接相应的结果集合（这在专用的图计算系统中不能支持）。\n\n图上不同的算法拥有各自的*节点集*，但可以共享同一个*边集*。可以实现数据的复用。此外，特定于图的索引数据结构可以在具有公共顶点和边集的图之间共享，从而减少存储开销并提高性能。\n\n#### 数据流操作实现图计算\n\nGraphX将分布式数据流框架中的图并行计算表示为一个由*连接（join）*阶段和*分组（group-by）*阶段组成的序列，这些阶段由*map（映射）*操作间隔。\n\n* 在**连接（join）**阶段， 节点和边连接形成一个*三元组视图（triplets view）*，包含边以及其源节点和目的节点的属性。\n\n* 在**分组（group-by）**阶段，三元组根据源顶点或目的顶点来构造每个节点的邻居节点，并进行聚合计算。\n\n*分组（group-by）*阶段*收集（gather）*发送到同一节点的消息，然后*映射（map）*操作*应用（apply）*聚合的消息来更新节点的属性，最后*连接（join）*阶段将新的节点属性*分散（scatters）*到所有相邻的节点。上面的过程构成了专用图计算系统的GAS模型。\n\n**迭代**的组合*连接（join）*和组（group-by）阶段以及数据并行映射（map）阶段，可以实现**Pregel抽象**。\n\n#### GraphX操作\n\n### GraphX的系统实现\n\n#### 分布式图表示\n\n#### 实现三元组视图\n\n#### 优化mrTriplets\n\n#### 其他优化\n\n* （1）基于内存的Shuffle；\n\n* （2）块状和柱状结构；\n\n* （3）可变的整数编码（一个字节中只使用7位来记录数据，最高的第8位用来记录是否需要另一个字节来记录该数字）。\n\n### 实验对比\n\n**（1）实验设置**\n\n* 实验环境：Amazon EC2，**16**个m2.4xlarge工作结点;每个结点有6个虚拟核，**64GB**内存。\n\n* 数据集：Twitter-2010，uk-2007-05。\n\n* 运行算法：PageRank，connected components。\n\n* 对比系统：Apache Spark 0.9.1，Apache Giraph 1.1，GraphLab 2.2 (PowerGraph)。\n\n**（2）对比其他系统**\n\n* 对比baseline的分布式数据流框架，性能高出最多一个数量级。\n\n* 与专门的图处理系统性能相当。\n\n**（3）GraphX性能**\n\n* 扩展性\n\n* 容错\n\n","tags":["graph processing system","distribute dataflow framework","paper reading"]},{"title":"Managing Large Dynamic Graphs Efficiently(SIGMOD 2012)","url":"/2019/11/20/Managing-Large-Dynamic-Graphs-Efficiently-SIGMOD-2012/","content":"\nMondal, Jayanta, and Amol Deshpande. \"Managing large dynamic graphs efficiently.\"2012 ACM SIGMOD. | [paper](https://dl.acm.org/citation.cfm?id=2213854) |\n\n### 网络图研究现状\n\n#### 网络图场景\n\n如今网络图越来越无处不在，如社交网络、通讯网络、金融交易网络、引文网络、基因调控网络、疾病传播网络、生态食品网络、传感器网络等等。网络数据甚至出现在普通的应用程序中，如电话呼叫数据、IP流量数据或包裹装运数据等。\n\n这些网络数据通常表示成一个*图（graph）*，然而目前缺乏一个良好的针对这种图结构的数据的数据管理系统，用以支持图上的查询与分析。\n\n并且，随着图规模的不断增长，这就需要使用并行和分布式的解决方案。然而，图操作不容易并行化，即使是对分布式图的简单查询也可能导致大量的网络遍历。\n\nMapReduce框架已经成为并行处理许多大型分析任务的框架。然而，MapReduce框架的目标是批量处理大量静态数据，既不支持实时数据摄取，也不支持实时查询。\n\n目前为止，有很多工作研究*单站点的图数据库系统*，他们通过对底层图的策略遍历，可以有效的执行特定类型的查询，比如*可达性（reachability）*，*关键字查询（keyword search queries）*，*子图模型匹配（subgraph pattern matching）*等。\n\n然而，*动态图数据的分布式管理*还没有得到很好的研究。在执行特定类型的查询或执行特定类型的分析方面也有一些工作，例如子图模式匹配、数据挖掘等，但这些工作要么关注点有限，要么就像Pegasus一样，是为了进行批处理。\n\n**本文的目标：：构建一个能够支持大规模动态变化的网络图的分布式图数据管理系统。**\n\n#### 图划分\n\n分布式图管理系统中一个关键的挑战是*图划分*，但是由于图结构的复杂连接，想要实现有效的图划分及其困难，尤其是在动态图的场景下。\n\n标准的**基于哈希的图划分方案**虽然能实现分布式集群的结点之间良好的负载均衡，但是通常会带来太多的*切割边（edge cut）*，因为大多数图查询或分析任务都需要遍历边来获取邻居信息。这不仅增加了查询延迟，还增加了整个网络通信，从而限制了系统的可扩展性。\n\n有很多工作研究更**复杂的图划分策略**。将一个图划分为大小相等的分区，同时最小化边切割是一个NP难的问题，目前也有一些实际的技术方案能够实现较好的图划分结果。但是，这些技术不能处理高度动态的图，因为节点访问模式和图结构本身可能会很快发生变化。\n\n更重要的是，在大多数实际应用中，图数据高度互联的特性意味着，通常不存在将边切割最小化的干净的不相交分区。特别是社交网络，由于社区结构重叠，存在高度连接的稠密分部，因此很难分割。\n\n#### 图复制\n\n一种替代的方案是**基于节点复制的方法**，其主要思想是复制图中的一些节点，以最小化分布式遍历的数量。\n\n**本地语义**，Pujol等人提出一种该方法的极端版本，即充分地复制图，以便对图中的每个节点，它的所有邻居都在本地出现(称为本地语义)。他们还使用主动复制策略（*push-on-change model*），即所有复制的数据都要即时保持最新。然而，这种方法需要非常高的、不必要的通信来保持副本的更新，Facebook使用一种替代的*pull-on-demand model*来减少更新的开销。\n\n为了能保证*本地语义*，其复制开销（即图中每个节点的平均副本数）通常特别大。对于一个Facebook数据集，只划分成8个分区，平均每个节点就需要大约2个副本。\n\n本文提出一种**混合自适应的复制策略**，减少复制开销，同时支持大规模图数据上的高效查询。\n\n### 本文系统设计\n\n#### 数据模型与查询模式\n\n#### 路由与存储\n\n本文在一个开源的KV存储系统Apache CouchDB的基础上实现了分布式图数据管理系统。\n\n#### 复制管理器\n\n本文使用一种新奇的**公平需求**的策略来指导复制的策略，公平需求策略可以使用一个阈值$\\tau \\leq 1$来刻画,可以简单表述为:**对于图上的每个节点，要求其所有邻居节点中不少于$\\tau$的部分存于本地**。当$\\tau = 1$时，就相当于*本地语义*。\n\n#### 复制决策\n\n根据*公平需求策略*的指导，需要对节点进行细粒度的选择性复制，并决定何时进行push或pull来更新副本信息，维护这些决策的开销通常也非常大。\n\n本文设计了一种**基于cluster的方案**，将具有相似访问模式的节点分组在一起，在不影响质量的情况下减少开销。该方案分析了决定复制什么，以及选择何时push或pull的问题，并提供了理论分析和有效的实用算法。该算法能够在每个节点上进行局部决策，能够在低负载期间更改决策，并/或错开决策的时间以避免性能的显著下降。\n\n### 实验对比\n\n**（1）实验设置**\n\n实验环境：Amazon EC2 infrastructure using 7 EC2 instances; 1 instance 1 core, and 1.7G of memory。\n（1个实例用于向系统发送更新和查询，其余6个用于存放图数据）\n\n数据集：使用*偏好连接模型*来构建图数据（已经被证明能够很好地模拟社交网络）。具体的生成过程为：每次添加一个节点，新增的节点*优先连接*到具有更高度（degree）的现有节点，即连接到现有节点的概率取决于该节点的度数。本文中大多数实验使用的图规模为**1.8M**个节点，**18M**条边。另外，本文选择了100个tweet数量足够的Twitter用户，并下载了他们的tweet以获得他们的访问跟踪，用于模拟用户的活动模式。\n\n运行算法：图查询。\n\n对比三种方法：((1) all-pull, where we do not do any replication, (2) all-push, where the nodes are replicated sufficient to guarantee no pulls would be needed (i.e., local semantics), and (3) hybrid, our hybrid approach.\n\n对比指标：所有服务器上交换消息计算的网络通信量，可以表示为推送和接收的消息的总量。\n\n**（2）直方图粒度的影响**\n\n**（3）带宽消耗和网络负载**\n\n**（4）改变Clusters的数量**\n\n**（5）改变Write-Read比率**\n\n**（6）改变公平阈值**\n\n**（7）改变图密度**\n\n**（8）Pull超时的影响**","tags":["graph processing system","paper reading","graph partition","graph vertices replication","dynamic graphs"]},{"title":"CGraph(ATC 2018) 并发图处理系统","url":"/2019/11/12/CGraph-ATC-2018-并发图处理系统/","content":"\nCGraph: A Correlations-aware Approach for Efficient Concurrent Iterative Graph Processing. Yu Zhang, Xiaofei Liao, Hai Jin, Liu Gu, Haikun Liu, Bingsheng He, Ligang He. ATC 2018 [presentation](https://https://www.usenix.org/conference/atc18/presentation/zhang-yu) | [paper](https://www.usenix.org/system/files/conference/atc18/atc18-zhang-yu.pdf) |\n\n### 并发图处理场景\n\n并发图处理（CGP）是指，大量的图算法并发的运行在同一平台上对底层同一个图数据进行处理，以对该图数据进行多方位的分析处理，获得各种目的性的分析结果。通过追踪一个大型社交网络上的信息发现，对同一个图数据在峰值的时候有超过20个计算任务在并发执行。\n\n这些任务之间独立运行，相互之间造成缓存干扰和内存墙的问题，限制了这些并发图计算任务的运行性能。实验发现，随着并发图处理任务的数量的增加，每个任务的平均执行时间明显增加，而增加的时间开销主要来源于更高的数据访问开销。因为并发图处理任务之间会竞争数据访问通道、内存和缓存。\n\n这些并发图处理任务对图数据的访问也存在关联性，包含空间关联性和时间关联性。\n\n（1）空间关联性：多个并发图分析任务需要访问和处理的图数据存在大量的交集。数据显示，平均每一轮并发图处理任务之间对图数据的访问的交集高达70%。\n\n（2）时间关联性：多个并发图分析任务可能在很短的时间内需要同时访问同一个图划分块。\n\n所以本文旨在考虑通过充分利用这些并发图处理任务对图数据的访问的空间/时间相关性，开发高效使用缓存/内存和数据访问通道的解决方案，以实现更高的吞吐量。\n\n### 相关工作\n\n现有的单机图处理系统，包括GraphChi，X-Stream，GridGraph，NXgraph和CLIP等，通常旨在优化使得有更高的顺序内存带宽，更好的数据局部性，减少冗余数据访问，更少的内存消耗等。他们主要针对单个图处理任务的场景能实现很好的性能，但对并发图处理场景运行效率并不高。\n\n### 以数据为中心的LTP模型\n\n以数据为中心的LTP包含三个阶段：Load-Trigger-Pushing。\n\n#### 加载图分块（Load）\n\n**数据分离**，一个图处理任务的数据通常包含四个部分，$D_J=(V_J,S_J,E_J,W_J)$，分别表示节点集，节点状态集，边集和边权重集。LTP模型中首先实现图结构数据和图处理任务状态数据的分离。即$D=(V,E,W)$为所有并发图计算任务的共享数据，被划分为多个块存储，$G=\\cup_iG^i$。各图处理任务分别有一个状态数据$S_J$。共享的图数据和各计算任务的状态数据通过多张KV表来存储。\n\n**图划分**，CGraph采用切割节点的方式，所有的边被均匀划分到不同的分块，跨越不同分块的节点建立master和mirror。这样能很好的达到各个图数据分块间的负载均衡，且避免各个图数据分块之间的通信，只有master和mirror之间需要通信。每个图分块的大小取决于CPU的核数N以及缓存容量的大小C，具体的值取满足下列公式的最大值，$P_g + \\frac{P_g}{s_g} \\times s_p \\times N + b \\leq C$。\n\n**动态图支持**，在动态图场景下，图数据结构会随着时间更新，比如增加或删除一些节点和边。这些更新只对在他们之后提交的图并发任务有效，CGraph通过一些快照（snapshots）来保存这些更新的增量信息，这样不同的并发图处理任务可以分别处理不同的快照信息。\n\n**图分块加载**，在每一轮计算中，各个并发图处理任务共享同一个图分块数据。所以每一轮，CGraph顺序加载有需求的图分区进入内存，这样在该图分区上有计算任务的图处理任务进行并发计算。\n\n但这种加载图分块的调度方式往往会造成加载的图数据未能被充分使用，原因有：\n\n1）有些度比较大的节点可能会比其他节点收敛更慢，而后面的几轮计算也需要加载进来那些已经收敛了的图数据；\n\n2）不同图分块的使用频率也是倾斜的并且会随时间变化；\n\n3）被加载的图分块可能只被很少（甚至一个）的计算任务需要，利用率很低。\n\n**基于核心子图的调度算法**，为了提高每个加载的图分块的利用率，CGraph提出一种基于核心子图的调度算法，其主要思想是：首先筛选出所有的核心节点（度数大于某阈值的节点）以及他们之间的连接边，构成一个核心子图；然后将这个子图的边放到几个大小相同的图分块中，其余的边被划分到其他大小相同的图分块中。这样，对核心节点的频繁加载和处理，带来的对相同分区中早期收敛顶点的加载开销更小。\n\n然后，CGraph给每个图分块一个优先级，优先调度优先级更高的图分块进入内存进行处理。\n\n1）当大多数任务需要对一个块进行处理时，给它更高的优先级；\n\n2）当一个块有更高的平均度或者更多的节点状态更新时，给它更高的优先级。\n\n具体的优先级的值计算如下：$Pri(P)=N(P) + \\theta \\cdot D(P) \\cdot C(P)$\n。其中，$\\theta$是一个使得第二部分小于1的数。\n\n通过这种方法，加载到缓存中的图分块可以服务尽可能多的图处理任务，而其他图分块在一段时间间隔后有更多的机会被更多的任务需要，通过降低平均数据访问成本进一步提高了吞吐量。\n\n实验表明，采用基于核心子图的调度算法最高能将执行时间减少到原来的60%。\n\n#### 触发（Trigger）各图处理任务的并发执行\n\n加载一个子图分区进入到内存以后，在该图分区上有计算任务的图处理任务进行并发计算。注意，任何新提交的作业只需要注明它在第一次迭代时要处理的分区，然后等待被触发来处理它们。当要并发处理的任务数量大于CPU的核数时，这些CGP作业被分配为不同的批处理。\n\n对于每个图分块的处理，不同CGP任务的计算负载往往是倾斜的，导致硬件的利用率较低。我们将当前图分块的私有表中未处理顶点数量最多的任务称为straggler。然后在逻辑上将straggler的未处理的顶点划分成多个片，并将他们分配到空闲的CPU核中。\n\n#### 数据推送（Pushing）实现master和mirror的状态同步\n\nCGraph在计算处理一个图分块时，是没有cache miss的，因为不同的图分块的节点之间是没有通信的。但是在不同图分块上，具有副本的顶点需要同步它们的状态。mirror节点需要推送（push）它的新状态到它的master节点，master节点获取到来自所有mirror节点的状态更新之后计算出当前轮最终的状态值，并推送到各个mirror节点。在这样的节点状态同步的过程中，私有状态表的许多图分区经常被加载到缓存中，导致较高的cache miss率。\n\n为了解决上述问题，CGraph将每个mirror节点的更新状态保存到一个缓冲区$S_j^{new}$中。在数据同步阶段再隐式发送到master节点以进行批量顶点状态同步。\n\n当有多个CGP作业要同步顶点状态时，对这些作业逐一进行，以减少资源争用，因为作业之间的状态数据没有数据共享。每个作业的各个节点的状态同步分批完成，首先将缓冲区$S_j^{new}$中的*状态更新项*按照其master节点所在的图分块的ID进行排序，这样很多状态更新变成了对同一分区的连续访问，我们只需要加载很少的的私有表分区。当一个master节点的连续更新完成时，就会得到当前迭代中这个顶点的最终状态。然后，这样的新值也可以直接批处理同步到他们对应的mirror节点。\n\n### 实验对比\n\n**（1）实验设置**\n\n实验环境：4-way 8-core Intel Xeon CPU E5-2670; each CPU has 20 MB LLC，64GB DRAM。\n\n数据集：Friendster，Twitter，uk2007，UK-Union，hyperlink14。\n\n运行算法：PageRank，SSSP，SCC，BFS。\n\n对比系统：CLIP，Nxgraph，Seraph。\n\n**（2）整体性能**\n\n**（3）可扩展性**\n\n**（4）动态图上的性能**","tags":["graph processing system","paper reading","concurrent graph processing"]},{"title":"KnightKing(SOSP 2019) 分布式随机游走系统","url":"/2019/11/04/KnightKing-SOSP-2019-分布式随机游走系统/","content":"\nKnightKing: A Fast Distributed Graph Random Walk Engine. Ke Yang, MingXing Zhang, Kang Chen, Xiaosong Ma, Yang Bai, Yong Jiang, SOSP 2019. | [Presentation](https://sosp19.rcs.uwaterloo.ca/videos/D3-S2-P2.mp4) | [Paper](http://delivery.acm.org/10.1145/3360000/3359634/p524-yang.pdf?ip=222.195.68.252&id=3359634&acc=OPENTOC&key=BF85BBA5741FDC6E%2EA4F9C023AC60E700%2E4D4702B0C3E38B35%2EC42B82B87617960C&__acm__=1572836886_72f8bbbe82d7289fac30d1fcc02962f6) |\n\n### 随机游走\n\n#### 应用场景\n\n随机游走是图数据分析和机器学习中一个重要的分析工具，可以利用图中节点之间的集成路径提取信息，经常被应用于一些重要的图分析、排序和嵌入式算法中，比如*PPR（personalized PageRank）*，*SimRank*，*DeepWalk*，*node2vec*等。\n\n这些算法既可以独立运行，也可以作为机器学习任务的预处理步骤。他们服务于各种应用场景，比如*点|边分类*，*社区检测*，*链接预测*，*图像处理*，*语言建模*，*知识发现*，*相似性测量*和*推荐系统*等。\n\n#### 执行过程\n\n图上的随机游走的执行过程为：\n\n1. 从图上出发$w$条walker，每条walker从一个特定的节点出发，walker之间独立的游走。\n\n2. 每一步，每条walker选取其所停留的当前节点的出边中随机采样选择一条出边，并沿着该出边跳转到下一跳。\n\n3. 每条walker在达到预设的终止条件时终止行走，一般为达到预设的长度或者预设的终止概率。\n\n一般来说，基于随机游走的算法的**输入**为一个图$G$。**输出**为随机游走过程中嵌入的计算结果，或者随机游走的路径信息。\n\n#### 关键步骤\n\n随机游走过程中的主要计算开销在于**边采样过程**，这也是不同的随机游走算法的不同之处，每个不同的随机游走算法定义其特有的**边转移概率**。随着随机游走的普及，边采样的逻辑也变得越来越复杂，近期的算法中更是提出了**动态采样**，即随机游走在每个节点处的采样概率不仅跟该节点的邻居信息有关，还与随机游走的状态有关。\n\n#### 复杂的随机游走\n\n复杂的随机游走算法能实现更高的灵活性，带来更好的性能，但随之带来的也有更大的采样的复杂度和计算开销。例如，一个很受欢迎的网络特征学习技术**node2vec**，包含*动态随机游走*和*跳跃语言模型构建*两个部分，而其在Spark上的实现据说98.8%的时间开销花在第一部分。而本文作者的实验表明，即使在Gemini的实现版本中，node2vec的执行会停滞在**边采样**的步骤，导致其在Twitter数据集中*每秒访问的节点个数*比BFS要低1434倍。\n\n#### 计算开销来源\n\n边采样过程中的巨大计算开销来源于它的*动态*特性，即在每一步进行随机边选择时，需要重新计算当前节点的所有出边的转移概率，这个计算开销随着当前节点度的增长而增长。而真实时间的图的度分布一般呈现幂律分布，所以造成各个节点的计算开销非常不均衡。而随机游走一般趋向于走到度数高的节点，这进一步加剧了采样开销。\n\n本文的作者对比考虑了两个真实时间的图Friendster和Twitter，他们有相似的平均度，分别为51和70，但是Twitter的度分布更加倾斜。在这两个图上执行node2vec，采用“Full-scan”的边采样方式，Twitter上的平均采样开销是Friendster的255倍，Twitter上进行一步walker转发平均需要将近访问$10^6$条边。\n\n#### 发展需求\n\n其他的随机游走也有如上所述的挑战，然而目前没有一个统一的随机游走计算框架，所有这些算法都需要用户自己在通用的图计算框架上实现，不仅工作量大而且性能也难以优化。\n\n这些随机游走算法都是以walk为中心的算法，但目前的图计算系统中大多采用以点或边为中心的计算模型，更关注于更新点和边的状态信息，随机游走中的“walker”通常作为消息信息被传递，此时walk信息很难记录，而那些依赖于walk状态来进行边采样的算法也很难实现。\n\n另外，现有的通用图计算系统中采用的有一些优化方案，对随机游走算法来说并不适用甚至反而降低随机游走算法的性能，比如*2-D图划分*和*GAS计算模型*。\n\n#### KnightKing概述\n\nKnightKing是一个*分布式随机游走计算引擎*，可以开成是传统分布式图计算引擎的随机游走副本。KnightKing采用以walk为中心的视角，用户自定义walk的边转移概率。与通用的图计算引擎类似，KnightKing隐藏了图划分，节点分配，通信和负载均衡等系统层面实现细节，只是给用户提供了一个“think-like-a-walker”的视角，用户可以灵活的添加可选的优化。\n\nKnightKing带来性能提升的核心在于**快速地选择下一跳**。\n\n（1）KnightKing首先引入了一个统一的**转移概率的定义**，引入*静态概率*和*动态概率*，有效的刻画各类随机游走算法。\n\n（2）然后采用**拒绝采样**的方式，避免了在进行边采样时遍历walker所在的当前节点的所有出边，大大减少了边采样的开销。对于相同的node2vec，换成KnightKing中的拒绝采样，采样一条边进行一步walk转发平均只需要0.79条边。另外，不同于现有的一些近似优化算法，KnightKing采用精确采样，提高性能的同时并没有损失准确度。\n\n（3）KnightKing也做了一些系统方面的优化工作，来支持以walk为中心的编程模型，并进一步提升随机游走的性能。\n\n### 统一的随机游走算法定义\n\n#### 随机游走分类\n\n一般随机游走的过程为：在给定图上出发一定数量的随机游走，每个出发于一个特定的节点，每条walker重复在当前节点的邻居节点中随机选择一个节点，并转发到该节点。不同的随机游走算法的不同就在于**邻居节点的选择**。\n\n根据当前节点的邻居节点们被选择的机会是不是相同的，可以分为**无偏**随机游走，即当前节点的各个邻居节点被选择的概率是相等的，和**有偏**随机游走，即当前节点的各个邻居节点被选择的概率取决于其权重或其他特征，各不相同。\n\n根据各个walker转发过程中在一条边上的转移概率是不是恒定的，又可以分为**静态**随机游走，即walker在一条边上的转移概率只与图结构有关，在walker的转发过程中保持恒定，和**动态**随机游走，即walker在一条边上的转移概率不仅与图结构有关，还与walker当前的状态有关，会随着walker的转发不断变化。所以对于动态随机游走来说，我们不再能够预计算出每条边的转移概率，而在每一步walker转发时都需要重新计算转移概率。\n\n我们可以进一步根据随机游走算法的转移概率考虑当walker最近路径的步长数来定义随机游走算法的**阶数**，**一阶随机游走**算法中，walker只知道当前节点，不知道之前访问的节点信息。**一阶随机游走**算法中，walker在选择下一跳的时候考虑上一步所在的节点。高于二阶（包含二阶）的*高阶随机游走为**动态**随机游走*。\n\n除了转移概率的不同，不同的随机游走算法也有不同的**终止策略**，常见的终止策略有（1）在walker走到一定步长后终止，（2）每个walker在每一步有一定的概率终止。\n\n#### 转移概率统一定义\n\n本文给出各类随机游走算法一个统一的非标准化（未归一）的转移概率的定义：\n\n*对于一个walker $w$，当前停留在节点$v$，该walker通过$v$的一条出边$e$转发到下一跳的概率为*\n\n$$P(e)=P_s(e) \\cdot P_d(e,v,w) \\cdot P_e(v,w).$$\n\n*其中，$P_s(e)$为静态分量，$P_d(e,v,w)$为动态分量，$P_e(v,w)$为扩展分量。*\n\n在这个统一定义的框架下，简单的随机游走算法就是有偏高阶算法的一个特例。比如简单的无偏静态算法，$P_s(e)=P_d(e,v,w)=1$。$P_e(v,w)$的设置独立于$P_s(e)$和$P_d(e,v,w)$，当一个walker满足终止条件时，设置$P_e(v,w)=0$。\n\n#### 典型随机游走算法\n\n下面介绍四个有代表性的随机游走算法，并给出他们在上述的统一的转移概率的定义下，各个分量的值。\n\n**（1）PPR**\n\nPPR，即个性化的PageRank（personalized PageRank），是经典的PageRank算法的一个更复杂的扩展版本。通用的PageRank算法通常是使用*幂法迭代计算*，但是PPR，尤其是*fully PPR*(为所有节点计算PPR)用幂法迭代计算需要非常高的时间和空间开销，对非常大的图来说通常难以承受。\n\n一个通常的做法是用基于随机游走的方案来计算得到近似值，考虑一个节点$v$的PPR可以从$v$出发进行一条很长的RWR，统计各个节点的访问概率，当walk的长度足够长，这里的访问概率就趋近于PPR值。这里我们考虑基于随机游走的fully PPR。\n\n考虑一个有向加权图，当前节点的一条出边被采样的概率正比于它的权重，即$P_s(e)=f(v,x)$，其中e是一条从$v$到$x$的边，$P_d(e,v,w)=1$。为了提升性能，我们通常将一条长walk拆成很多条短walk并行处理，每个短walk以一定的概率$P_t$终止，即在$P_t$的概率下，$P_e(v,w)=0$。（本文设置$P_t=0.0125 or 0.149$）\n\n这里，PPR是一个有偏静态随机游走算法。\n\n**（2）DeepWalk**\n\nDeepWalk是一个广泛应用于机器学习的一个图嵌入技术，利用语言建模技术来进行图分析。DeepWalk使用随机游走来产生很多的walk序列，每个节点作为一个节点，每个walk序列就构成一个句子，然后利用SkipGram语言模型来学习这些节点的潜在表征，这有很多应用场景，比如*多标签分类*，*链接预测*和*异常检测*。\n\n原始的DeepWalk是无偏的随机游走，这里我们扩展成加权图下有偏的随机游走，转移概率的设置与PPR相同，walk终止条件为走一定的walk长度。\n\n**（3）Meta-path**\n\nMeta-path用于捕捉节点和边之间的异构性背后蕴含的语义。Meta-path中，每条walker关联着一个**meta-path模式**，指定一个walker路径中边类型的模式。比如在一个论文发表网络图中，为了探索论文之间的引用关系，我们可以设置walker的初始节点为一个*作者*，设置meta-path模式为“isAuther -> citeBy -> autheredBy”。\n\n对于关联着meta-path模式$S$的一个walker，其在第$k$步的动态转移概率为$P_d(e,v,w)=1$, if $type(e)=S_{k mod |S|}$, otherwise $P_d(e,v,w)=0$。\n\n这里，Meta-path是一个动态的一阶随机游走算法。\n\n**（4）node2vec**\n\nnode2vec是一个高阶的随机游走算法，walker根据最近访问的历史信息来选择下一跳。node2vec与DeepWalk有相似的应用场景，但更加灵活。\n\n在一个无向图中，一个walker $w$，刚刚从节点$t$跳转到节点$v$，$x$是v的邻居节点，则node2vec中转移概率的动态分量可以表达为：\n\n$$P_d(e,v,w)=\\frac{1}{p}, if d_{tx}=0$$\n$$P_d(e,v,w)=1, if d_{tx}=1$$\n$$P_d(e,v,w)=\\frac{1}{q}, if d_{tx}=2$$\n\n这里，$p$和$q$是用户配置的参数，分别称为*回归参数*和*出入参数*，$d_{tx}$表示节点$t$到节点$x$的距离。考虑加权图的场景，$P_s(e)=f(v,x)$。\n\n这里，node2vec是一个有偏的动态二阶随机游走算法。\n\n### 发展现状\n\n目前已有一些研究工作用于加速边采样的过程，提升随机游走的性能，可以分为针对静态随机游走的优化和针对动态随机游走的优化。\n\n#### 静态随机游走\n\n**（1）逆转变换采样（ITS, Inverse Transform Sampling）**\n\n在已知所有边的采样概率的情况下，增加一个预处理过程，为每个节点构建一个数组$C$，用于存放其所有出边的转移概率的CFD（Cumulative Distribution Function），i.e.，$C[i]=\\sum_{j=0}^{i-1}{P_s(e_j)}$。设一个节点的出边数为$n$，则在随机游走进行边采样的时候，首先生成一个随机数$r \\in [0,C[n-1])$，并通过二分查找寻找最大的$i$使得$C[i]>r$，然后就选取$e_i$作为随机游走在该步采样的边。\n\nITS方法在预处理时需要$O(n)$的时间和空间用于构建CDF数据，在随机游走的每一步需要$O(log_2 n)$去采样一条边。\n\n**（2）别名法（alias method）**\n\n别名法也是在预处理阶段将每条边根据其转移概率拆分成一个或多个*片段（piece）*，每个图产生的总片段数不超过2n。然后将这些片段放入不同的*桶（bucket）*，使得每个桶的概率和是相等的。这些桶和他们的内容构成了一张*别名表（alias table）*。在随机游走进行边采样的时候，首先随机选择一个桶，然后在桶中根据每个片段的权重随机选择一个片段，将该片段所属的边作为随机游走在该步采样的边。\n\n别名法在预处理时需要$O(n)$的时间和空间用于构建别名表，在随机游走的每一步只需要$O(1)$去采样一条边。\n\n#### 动态随机游走\n\n上述两个方法针对静态随机游走时能提升较高的性能，但在动态随机游走的场景下，构建CDF数据或别名表都需要巨大的开销，比如针对Twitter数据集（11G）上的node2vec算法，就分别需要970TB或1.89PB的内存开销。所以动态随机游走不能用上述预处理的方法来加速边采样过程。目前也有一些加速动态随机游走算法的优化工作。\n\n**（1）针对专门算法的优化**\n有一些专门针对某一个特定算法的优化工作，比如针对meta-path算法，在预处理时，对每一种边的类型构建ITS数组或别名表。这样所有的边根据类型被划分成多个不相交集合，没有增加预处理额外的时间和空间开销。但这种方法并不能扩展到通用的动态随机游走算法。Fast-Node2Vec是针对node2vec的一个优化工作，通过缓存受欢迎节点的边列表来减少数据传输。这种方法牺牲了walker间的并发处理性能来节省内存开销。\n\n**（2）近似采样方法**\n\n也有一些近似采样算法来提升高阶动态随机游走的类型，比如node2vec-on-spark通过修剪掉高度节点，预处理时针对高度节点只选取其中的30条边。但即使牺牲了一定的准确度，这种方法依旧需要存储高达900|V|的转移概率。Fast-Node2Vec则是针对高度节点忽略去动态概率分布，只考虑其静态概率部分，以此来减少存储和采样开销。\n\n#### 系统优化工作\n\n目前针对随机游走算法的系统方面的工作只有DrunkardMob，但是它只考虑静态随机游走算法而且是设计在单机外核场景下，可扩展性受限。而其他通用的图计算系统是针对一些传统的图算法进行优化，并没有解决随机游走算法的问题。\n\n### KningKing的采样策略\n\nKningKing关键的创新点在于其边采样机制：动态概率分部采用拒绝采样，静态概率分部采用别名法。\n\n#### 拒绝采样\n\n**（1）无偏拒绝采样**\n*拒绝采样*开始用于通用的任意概率的采样，将一个一维的采样过程转换成一个二维的采样过程。具体对应到随机游走的边采样过程为：随机地产生一个位置$(x,y)$，其中$x$是从当前节点的出边中均匀随机选取的一条边e，$y \\in [0,Q(v)]$，其中$Q(v)$是当前节点的出边的动态转移概率的最大值。当$y \\leq P_d(e)$，则e被接收，当前walker通过e跳转。否则，e被拒绝，需要重新采样$(x,y)$并重复上面过程，直至walker成功跳转。\n\n通过拒绝采样的方式，消除了对当前节点所有出边的访问去获取他们的转移概率。一般只需要几次尝试就可以成功采样一条边（复杂度为$O(1)$）。大大减少了对高度节点的采样开销。\n\n**（2）有偏拒绝采样**\n\n有偏拒绝采样是指每条边的静态采样分部$P_s$是有偏的。一般是在加权图场景下采样概率正比于该条边的权重。无偏拒绝采样扩展到有偏拒绝采样，即在随机生成$(x,y)$时，通过$x$来进行边e的选取是有偏的。这里KnightKing使用ITS或别名法。\n\n**（3）采样复杂度**\n\n静态分部：预处理开销$O(n)$，计算开销$O(1)$（别名法）或$O(n)$（ITS）。\n动态分部：一次尝试开销$O(1)$。具体的开销取决于平均尝试的次数$E$，一般可以计算如下。\n\n$$E=\\frac{Q(v) \\cdot \\sum_{e \\in E_v}{P_s(e)}} {\\sum_{e \\in E_v}{P_s(e) \\cdot P_d(e)}}$$\n\n$E$并不直接关联节点的度，而是取决于$Q(v)$，所以我们可以通过减小$Q(v)$的值进一步优化拒绝采样的平均尝试次数，进而提升效率。\n\n#### 优化技术\n\n**（1）解决$P_d$中的异常值（outliers）**\n\n$P_d$中可能会存在几个异常大的值，从而增大了$Q(v)$的值（该节点所有出边的最大值），而大部分概率值都比较低。此时可以通过阶段异常值，拆成多个小的概率值的方式添加到所有出边的后面。这样减小$Q(v)$的值，而不改变每条边被采样到的概率（可以参考论文中的Figure 3）。\n\n**（2）预接收**\n\n可能该节点所有出边的概率值存在一个最小值$L(v)$，则当随机生成的$y \\leq L(v)$，我们直接接收当前节点，而不用去访问当前边获取它的$P_d$。\n\n### 计算流程和编程模型\n\n#### 随机游走的生命周期\n\nKnightKing是一个分布式随机游走系统，同时并发的处理很多条walker。walker根据其当前所在的节点被划分到不同的机器上，并行计算。为了同步协调很多节点上更新任务，KnightKing采用**基于迭代的计算模型**。不同于传统图计算系统上运行的确定性算法，KnightKing每一步的概率选择导致其每一轮计算流程与传统的图计算系统都有细微的差别。\n\n在处理高阶的随机游走时，每一轮迭代计算需要包含两轮消息传递：1）发送查询概率请求；2）收集查询结果。\n\n#### KnightKing API\n\n* 指定转移概率\n\n* 初始化和终止条件\n\n* walker状态\n\n### 系统优化\n\nKnightKing是一个分布式图计算系统，本文实现于2500行的C++源码，使用OpenMPI进行结点间通信。它的底层与分布式图引擎有很多共同之处，所以直接使用成熟的系统（Pregel和Gemini）中的基础设施和技术。\n\n#### 图的存储与划分\n\n存储：CSR格式。\n\n划分：KnightKing采样节点划分策略，每个节点连同它的出边被划分到集群中的一个机器结点（无向图中每条边存两次）。划分时使得每个机器上的节点数和边数的和尽量均匀。\n\n#### walker的执行与协调\n\n**（1）计算模型**\n\nwalker之间独立执行，会导致严重的不同步，而且带来巨大的网络通信开销。KnightKing采用BSP模型，协调同步各个walker的执行。\n\nwalker开始之前需要先进行一些初始化，包括*构建静态概率的别名表*，*设置动态概率的上下界*，*实例化walker*。\n\n每一轮采样**以walker为中心的计算模型**，包括*per-walker的消息生成*，*目标结点查找*，*消息批处理*，*all-to-all的消息传递*。还有一些额外的优化技术包括*缓冲池管理*和*流水线*，以使计算与通信重叠，实现更高的并行度。\n\n**（2）任务调度**\n\n每个结点上有两个线程专门用于消息传递，剩余线程用于walker转发。计算与消息传递并发执行，消息（walkers）被存放于一个共享队列供所有线程抓取，队列的长度一般设置为128。\n\n**（3）walker同步**\n\n即使采样BSP计算模型，大多数walker之间能够同步执行过程，但是仍然会存在一部分stragglers，拖慢整体的运行时间。stragglers的来源有以下两个部分：\n\n1）有些应用中，walker的执行长度本就各不相同，比如PPR，每条walker以一定的概率终止，不同walker终止在不同的步长。\n\n2）在基于动态转移概率的随机游走算法中，采用拒绝采样策略，被拒绝的walker只能待在原节点等到下一轮再尝试转发。\n\n本文统计了在LiveJournal上运行node2vec算法（每条walker运行80步后终止），发现有少部分stragglers落后很多，最慢的walker需要200多轮迭代计算才能完成计算，可以称之为“长尾”。\n\nKnightKing不能减少这些stragglers的迭代次数。但是长尾期间，总walker数量很少时（少于4000），可以通过降低的并发度（总共只使用3个线程，1个用于计算，2个用于通信）的方式来加速stragglers在每一轮的计算。实验表明该优化可以减少总运行时间的57.5%。\n\n### 实验\n\n**（1）实验设置**\n\n实验环境：8个结点的集群，每个结点装备Ubuntu 14.04系统，有2个8核处理器，20MB的L3级缓存，94GB的DRAM。\n\n数据集：LiveJournal，Friendster，Twitter，UK-Union。处理成无向加权图（每条边随机给定权重$[1,5)$）。\n\nRW应用：DeepWalk，PPR，Meta-path，node2vec。\n\n对比系统：Gemini。\n\n评估方法：执行时间。在非常慢的场景下，使用线性回归来推断运行时间。\n\n**（2）整体性能**\n\n**（3）图拓扑的敏感性**\n\n**（4）概率分布的敏感性**\n\n**（5）系统优化**\n","tags":["graph processing system","paper reading","random walks"]},{"title":"谷歌上网助手Ghelper","url":"/2019/10/30/谷歌上网助手Ghelper/","content":"\nGhelper是Chrome浏览器的一个插件，中文名为“谷歌上网助手”，可以让我们在无需翻墙的情况下，方便稳定的使用Google、Google学术等网站。\n\n### 下载插件\n\n（1）在[Chrome插件网](http://chromecj.com/)搜索“ghelper”，下载.crx文件。[推荐地址](http://chromecj.com/downloadstart.html#1501)\n\n（2）将扩展名改成.zip，然后解压到某本地目录下（我放在D:\\Google下），得到一个目录文件（不能删除）。\n\n### 安装插件\n\n（1）打开你的Chrome浏览器的 更多工具 --> 扩展程序页面。或者直接在网址输入: chrome://extensions/。\n\n（2）打开开发者模式。\n\n（3）点击 \"加载已解压的扩展程序\"，选择上一步解压得到的目录文件后确认添加。\n\n### 注册登录\n\n（1）安装完成后，在Chrome浏览器地址栏右侧可以看到Ghelper插件图标，点击出现登录页面。\n\n（2）新用户可以注册并通过邮箱激活，登录后即可访问Google。\n\n（3）新用户初次使用会有三天的VIP权限，三天后只可以使用免费权限，但免费权限就可以一直访问Google和Google学术等网站了。"},{"title":"可逆的马尔可夫链","url":"/2019/10/29/可逆的马尔可夫链/","content":"\n参考：\n\n[stochastic-I-Time-Reversibility](http://www.columbia.edu/~ks20/stochastic-I/stochastic-I-Time-Reversibility.pdf)\n\n[马尔可夫链](https://blog.csdn.net/u011898542/article/details/88018164)\n\n## 马尔可夫链\n\n### 定义\n\n（1） **马尔可夫链（Markov chain）**\n\n马尔可夫链是一组具有马尔可夫性质的离散随机变量的集合，可以表示为$X=\\{X_n:n>0\\}$，其中一维指数集为可数集，随机变量的取值都在可数集$S$内，即$X=s_i,s_i \\in S$，且下一步的随机变量$X_{t+1}$在给定其当前步随机变量$X_t$后与其余的随机变量条件独立，即随机变量的条件概率满足\n\n$$P(X_{t+1}|X_t, \\dots, X_1)=P(X_{t+1}|X_t)$$\n\n该性质即为**马尔可夫性质**，也称为“无记忆性”。\n\n可数集$S \\in Z$称为*状态空间（state space）*，马尔可夫链在状态空间内的取值称为*状态（state）*，马尔可夫链的指数集被称为*步（step）*。\n\n（2） **k-阶马尔可夫链（n-order Markov chain）**\n\nk-阶马尔可夫链拥有k阶的记忆性，可视为马尔可夫链的推广。类比马尔可夫链的定义，k-阶马尔可夫链满足如下条件\n\n$$P(X_{t+1}|X_t, \\dots, X_1)=P(X_{t+1}|X_t, \\dots, X_{t-k+1})$$\n\n（3） **马尔可夫过程（Markov process）**\n\n马尔可夫过程也被称为连续时间马尔可夫链，是马尔可夫链的推广，其状态空间是可数集，但一维指数集不再有可数集的限制，可以表示连续时间，其马尔可夫性质可表示为\n\n$$P(X(t+u)=s_j | X(t)=s_i) = P(X(u)| =s_j | X(0)=s_i))$$\n\n### 转移理论\n\n马尔可夫链中随机变量的状态随*步*的变化被称为演变（evolution）或转移（transition）。\n\n（1） **转移概率（transition probability**\n\n马尔可夫链中的状态间的单步转移概率可定义为随机变量间的条件概率$P_{ij}=P(X_{n+1}=s_j | X_n=s_i)$。\n\n（2） **转移矩阵（transition matrix）**\n\n若一个马尔可夫链的状态空间是有限的，则将所有状态间的的单步转移概率按矩阵排列，得到转移矩阵\n\n$$P=\n    \\begin{matrix}\n    P_{00} & P_{01} & \\dots & P_{0n} \\\\\n    P_{10} & P_{11} & \\dots & P_{1n} \\\\\n    \\dots & \\dots & \\dots & \\dots \\\\\n    P_{n0} & P_{n1} & \\dots & P_{nn} \\\\\n    \\end{matrix}\n$$\n\n马尔可夫链的转移矩阵是右随机矩阵（right stochastic matrix），每行元素之和等于1。在给定初始状态概率和状态转移矩阵，我们即可以确定该马尔可夫链的有限维分布（finite-dimensional distribution）。\n\n（3）**转移图（transition graph）**\n\n我们也可以通过转移图来刻画一个马尔可夫链的转移过程，一个状态表示为 一个节点，两个状态间的转移概率表示为一条加权边。\n\n对马尔可夫链中的两个状态$s_i$和$s_j$，若在转移图中存在一条路径$ik_0k_1\\dots j$，有$P_{i}P_{k_0}P_{k_1}\\dots P_{j}>0$，则称$s_i$到$s_j$**可达**，若$s_i$和$s_j$互相可达，则称$s_i$和$s_j$之间**连通**。连通是一组等价关系，因此多个相互连通的状态可以构建一个等价类，包含尽可能多状态的等价类称为**连通类**。\n\n给定状态空间的一个子集，若马尔可夫链进入该子集后无法离开，则称该子集为**闭合集**。若一个闭合集中只有一个状态，则该状态是**吸收态**。若一个马尔可夫链从任意状态出发最终都会进入吸收态，则称该马尔可夫链为**吸收马尔可夫链（absorbing Markov chain）**。\n\n\n### 性质\n\n不同的马尔可夫链可能会具备不同的性质。\n\n（1）**不可约性（irreducibility）**\n\n如果一个马尔可夫链的状态空间仅有一个连通类，即状态空间的全体成员，则该马尔可夫链具有不可约性。马尔可夫链的不可约性意味着在其演变过程中，随机变量可以在任意状态间转移。\n\n（2）**重现性（recurrence）**\n\n若马尔可夫链在到达一个状态后，在演变中能反复回到该状态，则该状态具有**重现性**，否则该状态具有**瞬变性（transience）**。若状态$s_i$具有重现性，则可计算其平均重现时间（mean recurrence time）$E(T_i)$，若$E(T_i)<\\infty$，则该状态是**正重现的（positive recurrent）**，若$E(T_i)=\\infty$，则该状态是**零重现的（null recurrent）**，意味着马尔可夫链两次访问该状态的时间间隔的期望是正无穷。\n\n推论：若有限个状态的马尔可夫链是不可约的，则其所有状态是正重现的。\n\n（3）**周期性（periodicity）**\n\n一个正重现的马尔可夫链可能具有周期性，即在其演变中，马尔可夫链能够按大于1的周期重现其状态。\n\n推论：若不可约的马尔可夫链有周期性状态A，则该马尔可夫链的所有状态为周期性状态。\n\n推论：若状态A与状态B连通，则A与B周期相同。\n\n（4）**遍历性（ergodicity）**\n\n若马尔可夫链的一个状态是正重现的和非周期的，则该状态具有遍历性。若一个马尔可夫链是不可约的，且有某个状态是遍历的，则该马尔可夫链的所有状态都是遍历的，被称为遍历链。\n\n推论：若多个状态的马尔可夫链包含吸收态，则该马尔可夫链不是遍历链。\n\n### 稳态分析\n\n（1）**平稳分布（stationary distribution）**\n\n若一个马尔可夫链的状态空间存在概率分布\n\n$$\\pi=\\pi P$$\n\n则称$\\pi$是该马尔可夫链的平稳分布。\n\n对于一个不可约的马尔可夫链，当且仅当其存在唯一平稳分布，该马尔可夫链是正重现的，且平稳分布有如下表示\n\n$$\\pi(s_i)= \\frac{1}{E(T_i)}$$\n\n$E(T_i)$是状态$s_i$的平均重现时间。\n\n马尔可夫链存在平稳分布的充要条件是其存在正重现状态。若一个马尔可夫链包含多个由正重现状态组成的连通类，则每个连通类都拥有一个平稳分布，且演变得到的稳态取决于初始分布。\n\n（2）**极限分布（limiting distribution）**\n\n若一个马尔可夫链的状态空间存在概率分布\n\n$$\\lim_{n \\to \\infty}{p(X_n=s_i)}=\\pi(s_i)$$\n\n则称$\\pi$是该马尔可夫链的极限分布,极限分布与初始分布无关。\n\n*极限分布一定是平稳分布，但反之不成立，例如周期性的马尔可夫链可能具有平稳分布，但周期性马尔可夫链不收敛于任何分布，其平稳分布不是极限分布。*\n\n（3）**极限定理（limiting theorem）**\n\n两个独立的遍历链各自给定任意初始分布，如果他们有相同的转移矩阵，那么当时间步趋于无穷时，两者极限分布间的差异趋于0。该定理表明若马尔可夫链是遍历的，则其极限分布是平稳分布。\n\n（4）**遍历定理（ergodic theorem）**\n\n若一个马尔可夫链为遍历链，其对某一状态的访问次数与时间步的比值，在时间步趋于无穷时趋近于平均重现时间的倒数，即该状态的极限平稳分布。\n\n遍历定理的证明依赖于强大数定律（Strong Law of Large Numbers, SLLN），表明遍历链无论初始分布如何，在经过足够长的演变后，对其中一个随机变量进行多次观测和对多个随机变量进行一次观测都可以得到极限分布的近似。由于遍历链满足极限定理和遍历定理，因此MCMC通过构建遍历链以确保其在迭代中收敛于平稳分布。\n\n（5）**平稳马尔可夫链（stationary Markov chain）**\n\n若一个马尔可夫链拥有唯一的平稳分布且极限分布收敛于平稳分布，则该马尔可夫链是**平稳马尔可夫链**，也被称为**齐次马尔可夫链（time-homogeneous Markov chain）**。\n\n（6）**可逆马尔可夫链（reversible Markov chain）**\n\n一个平稳马尔可夫链也是可逆马尔可夫链，其平稳分布对任意两个状态满足\n\n$$\\pi(s_i) P(X_{n+1}=s_j|X_n=s_i) = \\pi(s_j) P(X_{n+1}=s_i|X_n=s_j)$$\n\n或者简单写成 $\\pi_i P_{ij}=\\pi_j P_{ji}$。\n\n*平稳马尔可夫链与可逆马尔可夫链互为充要条件。？？？*\n\n### 求解稳态分布\n\n（1）**通过马尔可夫链的可逆性求解其平稳分布**\n\n可逆的马尔可夫链一定存在平稳状态，且可以通过其可逆性，求解其平稳分布。\n\n**例（无向加权连通图上的随机游走）**考虑一个n个节点的无向连通图，任意两个节点$i$和$j$之间若存在一条边，其权重$w_{ij}=w_{ji}>0$，否则$w_{ij}=w_{ji}=0$。在该图上进行随机游走，节点$i$到$j$的状态转移概率为\n\n$$P_{ij}=\\frac{w_{ij}}{\\sum_{k}{w_{ik}}}$$\n\n该随机游走所刻画的马尔可夫链为可逆的马尔可夫链。所以任意两个节点$i$和$j$有\n\n$$\\pi_i P_{ij}=\\pi_j P_{ji} \\to \\frac{\\pi_i}{\\sum_{k}{w_{ik}}} = \\frac{\\pi_j}{\\sum_{k}{w_{jk}}}$$\n\n所以，对所有的节点$i$有\n\n$$\\frac{\\pi_i}{\\sum_{k}{w_{ik}}} = C \\to \\pi_i=C\\sum_k{w_{ik}}$$\n\n因为$\\pi$是一个在所以状态上的概率分布，有$\\sum_i{\\pi_i}=1$，所以\n\n$$\\sum_i{(C\\sum_k{w_{ik}})}=1 \\to C=\\frac{1}{\\sum_i{\\sum_k{w_{ik}}}}$$\n\n最终求得该马尔可夫链的平稳分布为\n\n$$\\pi_i=\\frac{\\sum_k{w_{ik}}}{\\sum_i{\\sum_k{w_{ik}}}}$$\n\n若是考虑无权图是场景，即任意两个节点$i$和$j$之间若存在一条边，其权重$w_{ij}=w_{ji}=1$，记$d_i$为节点$i$的度，则其平稳分布为\n\n$$\\pi_i=\\frac{\\sum_k{w_{ik}}}{\\sum_i{\\sum_k{w_{ik}}}} = \\frac{d_i}{\\sum_i{d_i}} = \\frac{d_i}{2|E|}$$","tags":["theoretical analysis","random walk"]},{"title":"知识图谱和图数据库","url":"/2019/10/23/知识图谱和图数据库/","content":"\n参考博客：\n\n[知新温故，从知识图谱到图数据库](https://blog.csdn.net/wireless_com/article/details/86486289)\n\n[知识图谱-浅谈RDF、OWL、SPARQL](https://www.jianshu.com/p/9e2bfa9a5a06)\n\n### 知识图谱\n\n#### 需求与定义\n深度学习、机器学习等人工智能技术，要在行业中得到应用，首先要对行业已有的知识有足够的认知。知识图谱可以用来描述真实世界中存在的各种实体和概念，以及他们之间的强关系。构建和完善知识图谱是事物分析学习的基础。\n\n知识图谱可以理解成是由很多知识点和它们之间的关系连接构成的语义网络。也可以简单的将一个知识图谱理解成一个*多关系图*，即图中包含多种类型的节点和多种类型的边。在知识图谱里，通常用*实体*来表达图里的节点，即现实世界中的事物，用*关系*来表达图里的边，即不同事物之间的某种联系，实体和关系也通常拥有各自的属性。\n\n#### 设计与构建\n设计某行业的知识图谱需要对业务本身有足够的理解，从业务逻辑出发，并考虑业务未来可能的变化。要把知识图谱设计成轻量级的存储载体，决定哪些数据需要放在知识图谱中。\n\n构建知识图谱，首先要进行*数据抽取*，即把数据从不同的数据源中提取出来，进行统一的管理，这其中需要用到自然语言处理（Natural Language Processing，NLP）中的一些技术，包括实体命名识别、关系抽取、实体统一、指代消解等。\n\n#### 存储方式\n知识图谱主要有两种存储方式：RDF和图数据库。\n\n##### RDF\n\nRDF，即资源描述框架（Resource Description Framework），是W3C提倡的一个数据模型，用来描述万维网上的资源及其相互间的关系。RDF数据模型的核心包括资源（resource）、属性（property）、RDF陈述（RDF statement）。\n\n*资源*，表示一个具体的事物或抽象的概念。每个资源拥有一个统一资源标识符（URI）来标识。\n\n*属性*，表示资源之间的联系，每个属性也使用唯一的URI来标识。\n\n*RDF陈述*，描述某个资源特定属性及其属性值，表示为（主语——谓语——宾语）的三元组结构。\n\n*RDF图*，由很多RDF三元组组成的一个集合可以构成一个RDF图。RDF图也可以看成是节点和边均带有标签的有向图结构。\n\n##### 图数据库\n\n图数据库是非关系型数据库（Not Only Structured Query Language, NoSQL）的一种，重点描述数据之间关系的数据库。\n\nRDF与图数据库的区别在于，RDF一个重要的设计原则是数据的易发布以及共享，图数据库则把重点放在了高效的图查询和搜索上。其次，RDF以三元组的方式来存储数据而且不包含属性信息，但图数据库一般以属性图为基本的表示形式，所以实体和关系可以包含属性，这就意味着更容易表达现实的业务场景。RDF常应用于学术场景，而图数据库常用于工业场景。\n\n### 图数据库\n\n#### 关系型数据库\n传统的关系型数据库更注重刻画实体内部的属性，实体与实体之间的关系通常都是利用外键来实现，将所有的数据用竖立的堆栈表示，并且保持它们直接的关系，在求解关系的时候通常需要join操作，而join操作通常又是耗时的。常常被优化用于聚合数据，而非高度关联的数据。对于高度关联的数据存储与分析就需要求助于NoSQL了。\n\n#### 非关系型数据库\n非关系型数据库（Not Only Structured Query Language, NoSQL）可以分为4类：key-value，文档型，列存储和图数据库。\n\nKey-Value模型适合用于简单的数据或者列表。当数据之间不断交互关联时，实际上更需要一张图。\n\n文档型NoSQL用来管理文档。在传统的数据库中，信息被分割成离散的数据段，而在文档数据库中，文档是处理信息的基本单位。文档可以很长，可以很复杂，可以是无结构的，与字处理文档类似。一个文档相当于关系数据库中的一条记录。文档型NoSQL用文档进行层次划分，而自由的数据规划也很容易被表示成一颗树。成长为一张图的话，文档之间的关联需要更有代表性的数据结构来存储。\n\n列存储。\n\n从应用开发的角度看，这些NoSQL数据库不处理关系，没有数据结构建模或存储数据关系，没有查询结构支持些数据关系。而且，在应用中连接数据同样需要JOIN，操作 对事务没有 ACID 的支持。因此，这三种 NoSQL 数据库也不适用于有实时价值的数据关系。\n\n#### 图数据库\n图数据库是基于数学里图论的思想和算法而实现的高效处理复杂关系网络的数据库。图形数据库善于高效处理大量的、复杂的、互连的、多变的数据，计算效率远远高于传统的关系型数据库。\n\n图中每个节点代表一个对象，节点之间的连线代表对象之间的关系。节点可带标签，节点和关系都可以带若干属性。关系可以将节点组织成任意的结构，允许一张图被组织成一个列表，一棵树，一张地图，或者一个复杂的实体。这个实体本身也是由复杂的，关系高度关联的结构组成。\n\n#### 现有的图数据库\n\nNeo4j\n\ntitan\n\narangoDB\n\nOrientDB\n\nGUN","tags":["graph database"]},{"title":"CNARW 基于公共邻居感知的快速随机游走","url":"/2019/10/18/CNARW-基于公共邻居感知的快速随机游走/","content":"\n[Yongkun Li, Zhiyong Wu, Shuai Lin, Hong Xie, Min Lv, Yinlong Xu, John C.S. Lui. \"Walking with Perception: Efficient Random Walk Sampling via Common Neighbor Awareness\". 35th IEEE International Conference on Data Engineering (ICDE), Macau, SAR, China, April 2019.](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8731555)\n\n### 图中心性分析\n\n#### 应用场景\n\n近年来，对社交网络分析能更准确的进行一些商业活动，比如病毒式营销和产品推荐等。通过分析图上的各种图中心性可以获取社交网络中用户的属性，进而用来促进商品营销。可以通过下面具体的两个例子来直观的说明。\n\n（1）**网络平台投资**，根据病毒式营销中的“口碑效应（word-of-mouth）”，一个用户购买商品时可能会受到其朋友的影响而去买同一件商品。所以利用在线社交网络（OSN）可以很好的进行商品营销。不同的OSN会呈现不同的潜力，比如不同的OSN中用户的活跃性和影响力会有所不同。所以一个商家来说，**选择哪个网络平台进行投资能吸引到最多的用户购买商品？**这个问题可以通过*估算OSN中所有*用户对*之间的平均相似性*来衡量。\n\n（2）**病毒式营销中的捆绑策略**，将多个商品在一起打折捆绑销售也是一种常见的营销策略。但是**具体选择哪些商品放在一起捆绑销售能带来最大的销售额**。这个问题可以通过*估算每个商品在用户之间的兴趣分布，捆绑有相似分布的商品*来解决。\n\n#### 计算挑战\n\n想要准确的计算图上的这些中心性不是件简单的事情，主要有如下挑战。\n\n（1）OSN的图规模通常很大，例如Facebook的用户数已经超过20亿。\n\n（2）为了保护用户的隐私，很多OSN只允许第三方代理通过固定的速度受限的API接口访问网络数据。\n\n为了分析这些大规模图数据，**图采样**是个常用的技术，通过分析采样的一些有代表性的样本，而避免遍历整个网络数据，这样大大减少了对网络的访问开销。\n\n### 随机游走采样\n\n随机游走采样是这类应用场景的一个主流采样算法，因为它扩展性强且实施简单。\n\n#### 简单随机游走\n\n考虑一个无向连通图$G(V,E)$，$N(v)$表示图中节点v的邻居集合，$deg(v)=|N(v)|$表示节点v的度。\n\n图上简单随机游走过程是：首先从图中随机选取一个节点，然后重复从当前节点中随机挑选一个它的邻居节点进行跳转。\n\n这个过程可以看成是一个有限的马尔科夫链，每一步访问的节点id就马尔科夫链的状态。每一步的状态转移概率可以表示成一个$|V|\\times |V|$的概率转移矩阵$P$，$P_{uv}$为从节点u通过一步random walk走到节点v的概率。简单随机游走（Simple Random Walk, SRW）的状态转移矩阵可以表示为$P_{uv}=1/deg(u), \\quad if \\quad v \\in N(u)$，否则$P_{uv}=0$。\n<!-- $P_{uv}= \\begin{cases} 1/deg(u) & \\text{if v \\in N(u)}\\\\0 & \\text{otherwise} \\end{cases}$ -->\n\n很多步以后达到收敛状态，即随机游走访问图中每个节点的概率呈现稳态分布。SRW收敛后的稳态分布可以表示成：$\\pi (u)=deg(u)/(2|E|)$。\n\n#### 随机游走采样\n\n随机游走的采样是在随机游走收敛以后开始采样收集样本。根据收集到的样本和收敛后的稳态分布，我们就可以对感兴趣的一些图测量指标进行无偏估计。\n\n##### 收集样本\n\n收集样本节点时，一般有两种方法：（1）*连续采样*，在图中只开启一条random walk，收敛以后持续采集样本直到收集到足够的样本节点。（2）*独立采样*，在图中同时开启多条random walk，每条walk收敛以后只采集一个样本节点。这两种方法都需要在random walk收敛以后才能开始收集样本，然后根据稳态分布进行无偏估计。\n\n##### 无偏估计\n\n假定图上的一个测量指标可以表示成一个函数$f:V \\to R$，在一个达到稳态分布$\\pi$的随机游走上收集到的足够的样本上应用函数$f$，即可得到一个估计值$E_\\pi[f] \\stackrel{\\mathrm{\\Delta}}{=} \\sum_{u \\in V}{f(u)\\pi(u)}$。该估计值的准确性可由**强大数定律（the Strong Law of Large Numbers, SLLN）**保证。\n\n#### 加速随机游走收敛\n\n从开始到达稳态分布的持续时间称为“the burn-in period”，在真实的OSN中，这个阶段通常需要一个很大的计算开销，因为通常需要很多步以后才能达到收敛。在给定一个采样预算（总共访问的节点数）的情况下，除去收敛开销，我们只能采样少量的有代表性的样本，因此分析的准确性就会收到影响。所以随机游走采样一个重要的问题就是：**如何加速随机游走在大规模图上的收敛？**\n\n现在有两类加速随机游走收敛的方法：\n\n（1）增加图的导通性（conductance），这类方法通常需要全局的图信息，所以在现实场景中通常不可行。\n\n（2）修改每一步walk的转移概率，这类方法通常是利用walk的历史信息，并且只需要访问少量的局部图信息。\n\n### CNARW\n\n这篇论文跟随第二类加速随机游走收敛的方法，提出公共邻居感知的随机游走（Common Neighbor Aware Random Walk）CNARW，通过利用walk前一步访问过的节点信息来优化下一步跳转的邻居选择。具体的，CNARW考虑当前节点和下一跳候选节点的公共邻居数量来进行下一跳的邻居选择。\n\n#### 主旨思想\n\n考虑简单随机游走收敛慢的主要原因：一般的社交网络都有很高的*聚类*特性，即图中形成很多的社区结构，社区内部的节点连接紧密，社区之间连接稀疏。所以简单随机游走的过程中，walker均匀随机地选取当前节点的一个邻居跳转，大概率会选到社区内部的节点，而且很容易陷入当前子图，只在社区内部反复游走在已经访问过的节点，只有很小的概率能走出当前的子图，从而探索到全局的图信息。这大大减慢了简单随机游走的收敛速度。\n\n所以为了加速随机游走的收敛，我们需要尽量减少对已经访问过的节点的频繁的再次访问。该论文提出，在每一步随机游走选择的时候，不同于简单随机游走的均匀随机选择，我们给访问过的节点小一点的访问概率，而给有更大机会能探索更多未访问的节点的那些节点大一点的访问概率。该论文根据下一跳候选节点的信息以及一些历史访问信息，重新设置每一步随机游走的转移概率，具体考虑如下两个方面。\n\n（1）若一个候选节点的*度*，即邻居节点数，越大，则它可能访问到更多未访问的节点的概率也就越大。\n\n（2）若一个候选节点与当前节点的公共邻居数越少，则随机游走通过该候选节点再次回到当前节点的概率也就越小。\n\n所以，在各个候选节点中，我们应该给度数高且与当前节点公共邻居数少的节点更大的转移概率。通过这种加权游走的策略，CNARW可以更快的收敛。\n\n\n#### 算法设计\n\n下面介绍通过一些公式化的表示和理论支持而形成的CNARW的具体算法设计。\n\n**（1）首先引入集合导通性（set conductance）**\n\n**定义：集合导通性（set conductance）**，用$G=(V,E)$表示一个无向图，$C \\in V$是图中的一个节点集，集合$C$的导通性$\\phi (C)$定义为\n$$\\phi (C)=\\phi (C,V-C)=|E_{C,V-C}| / Vol(C),$$\n其中$E_{C,V-C}={(u,v) | u \\in C, v \\in V-C}$，$Vol(C)=\\sum_{u \\in C}{deg(u)} $。\n\n集合导通性可以看成是节点集合$C$和它的差集之间的边数除以$C$内部的边数。一个节点集与图中其他节点的连接数越多，节点集内部的连接数越少，则该节点集的导通性越大，随机游走被困在该节点集中可能性也就越小。\n\n**（2）接下来公式化下一跳的节点选择**\n\n假设随机游走当前停留在节点$u$，定义$S={u} \\cup N(u)$为包含当前节点和它的邻居节点的*边界节点集（frontier nodes）*。则我们可以用$\\phi(S)$来表示随机游走可能被困在$S$的概率程度。而其中一个候选节点$v$对$\\phi(S)$的贡献度可以表示为$\\Delta \\phi_v = \\phi(S)-\\phi(S_{-v})$，其中$\\phi(S_{-v})=S \\setminus {v}$。\n\n$$\\Delta \\phi_v = \\frac{ (1-\\phi(S))-2(C_{uv}+1)/deg(v) }{ (\\sum_{i \\in S}{deg(i)})/deg(v)-1 },$$\n\n其中$deg(v)$表示节点$v$的度，$C_{uv}$表示节点$u$和节点$v$的公共邻居数。在$deg(v)$不变的情况下，$C_{uv}$越大，则$\\Delta \\phi_v$越小。而在固定$C_{uv}$不变的情况下，$deg(v)$越大，则$\\Delta \\phi_v$也越大。\n\n对于每个候选节点$v \\in N(u)$，$\\Delta \\phi_v$可以作为下一跳节点选择合适程度的测量指标。$\\Delta \\phi_v$越大，即度数高且与当前节点公共邻居数少的节点，CNARW给与节点$v$在下一跳邻居选择中更高的权重。\n\n**（3）转移矩阵的设计**\n\n直觉来说，可以将节点$u$到节点$v$的转移概率$P_{uv}$设置为一个正比于$\\Delta \\phi_v$的值，比如为了避免$\\Delta \\phi_v$的复杂计算，我们可以简单的设置为$P_{uv}=1-\\frac{C_{uv}}{deg(v)}$。\n\n但是为了保证随机游走的*可逆性（reversible）*，从而能够简单地获得它的稳态分布。因此，我们设计转移概率时需要保证*对称性（symmetric）*，即$P_{uv}=P_{vu}$，具体CNARW设置为\n\n$$P_{uv} \\varpropto 1-\\frac{C_{uv}}{min(deg(u),deg(v))}.$$\n\n我们也可以考虑其他的形式，比如上式中的分母也可以替换成$deg(u)+deg(v)$或者$max(deg(u),deg(v))$，也能保证对称性。但是使用$min(deg(u),deg(v))$可以避免当$deg(u)$很大时，$P_{uv}$之间差别很小的情况。\n\n**（4）随机游走过程**\n\n为了实现满足上述转移概率的随机游走，且减少计算开销，CNARW采用了一种*带拒绝的随机游走策略*。具体的，在每一步随机游走时：\n\n1）我们首先从当前节点$u$的邻居节点$N(u)$中随机均匀的选取一个候选节点$v$；\n\n2）然后计算节点$v$的接收概率，表示为$q_{uv}=1-\\frac{C_{uv}}{min(deg(u),deg(v))}$；\n\n3）我们以$q_{uv}$的概率接收然后将随机游走跳转到节点$v$，以$1-q_{uv}$拒绝，并重新回到1）选取一个候选节点；\n\n4）重复上述过程直至随机游走成功转发；\n\n这种*带拒绝的随机游走策略*的好处是我们只需要访问被接收的节点$v$及其之前访问过的节点信息，而不需要访问节点$u$的所有邻居，从而减少了查询开销。\n\n**（5）转移概率归一化**\n\n上述的随机游走过程中，会有一定的概率$P_{uu}= 1 - \\frac{1}{deg(u)}\\sum_{v \\in N(u)}{(1-\\frac{C_{uv}}{min(deg(u),deg(v))})}$跳回到当前节点。为了避免这种情况，我们对转移概率进行归一化矫正，$P_{uv}={p'_{uv}/(1-p'_{uv})}$。\n\n#### 稳态分布\n\n**定理（唯一存在性）**：给定无向连通图G(V,E)，在G上运行CNARW存在唯一的一个稳态分布。（不可约）\n\n**定理（节点的稳态分布）**：CNARW的稳态分布$\\pi$满足：$\\frac{\\pi(u)}{\\pi(v)}=\\frac{deg(u)(1-p'_{uu})}{deg(v)(1-p'_{vv})}$，所以我们可以得出$\\pi(u)=Z \\times deg(u) \\times (1-p'_{uu})$，其中Z是一个归一化的常数。\n\n**定理（边的稳态分布）**：CNARW收敛后，$\\pi(e_{uv})=\\pi(u) \\times P_{uv}$。\n\n#### 扩展利用更多历史节点信息\n\n上述介绍的CNARW算法是只利用了当前节点（一步历史）的信息，直观来看，考虑更多的历史信息可以进一步加速随机游走的收敛。所以该论文中也考虑了扩展到考虑多个历史访问节点的的信息，来设计随机游走的转移概率。\n\n我们用H来表示考虑的之前访问的节点个数，$H=0$和$H=1$分别对应SRW和CNARW的场景。$H \\geq 2$时，重新定义*边界节点集（frontier set）*$S = N(x_H) \\cup N(x_{H-1}) \\cup \\dots \\cup N(x_2) \\cup N(u)$。此时，候选节点$v$的对$\\phi(S)$的贡献度$\\Delta \\phi_v^H$为：\n\n$$\\Delta \\phi_v^H = \\frac{((1-\\phi(S))-2(C_{Sv}+1)/deg(v) )}{(\\sum_{i \\in S}{deg(i)})/deg(v)-1}.$$\n\n该论文在实验中测试了H对收敛速度的影响，结果表明$H=1$时的性能提升就已经足够和有力。更大的H带来的进一步的性能提升并不显著。\n\n### 基于CNARW的无偏采样\n\n基于CNARW，这篇论文提出了一种采样算法，并且提供了高效的无偏估计方法并且提供理论证明来保证无偏估计的准确度。\n\n#### 无偏点采样\n\n#### 无偏边采样\n\n\n### 实验 \n\n（1）实验环境和数据集。\n\n服务器：2 Intel Xeon E5-2650 2.60GHz CPUs & 64GB RAM。\n\n数据集：（a）4个大规模数据集：Google Plus, Flickr, DBLP and LiveJournal；（b）3个小规模数据集： Facebook, CaGaQc, and Phy1（用于计算第二大特征值）。\n\n对比算法：SRW, NBRW, CNRW。\n\n（2）性能指标定义。\n\n（3）收敛速度，包括实验统计的收敛需要的步数以及理论计算的第二大特征值。\n\n（4）估算误差和查询开销。\n\n（5）H的影响。\n\n（6）转移矩阵的设计。\n\n（7）具体应用场景。\n\n* 网络平台投资\n\n* 病毒式营销中的捆绑策略\n\n实验结果表明，（1）CNARW最多能将当前的随机游走算法SRW，NBRW，CNRW的收敛所需的步数减少71.9%。（2）在实现相同的准确度的情况下，CNARW最多能减少35.7%的查询开销。\n\n<!-- ### 扩展工作\n\n（1）考虑其他转移概率设计（为什么需要对称性）。\n\n（2）扩展到有向图。\n\n（3）理论证明CNARW的收敛速度快，即第二大特征值小。\n\n（4）应用NBRW和CNRW的设计优化到CNARW上，进一步加速收敛。\n\n（5）考虑更多大数据集。\n\n### review意见\n\n（1）将所有符号列成一张表。\n\n（2）为什么要采用带拒绝的采样策略？\n\n（3）转移概率设计为什么需要对称？\n\n（4）* 理论证明收敛速度。\n\n（5）相对误差定义。\n\n（6）使用真实数据集，大数据集，比如Epinion。\n\n（7）提出的算法似乎不能在有向图上运行。\n\n（8）有些描述不清晰，比如X_{t}是什么？\n\n（9）更多的讨论MHRW。 -->\n\n","tags":["random walks","theoretical analysis","graph sampling"]},{"title":"Deeper Inside PageRank","url":"/2019/10/17/Deeper-Inside-PageRank/","content":"\n[Deeper Inside PageRank, Langville A;Meyer C, Internet Mathematics 2004](https://www.internetmathematicsjournal.com/article/1388)\n\n这是一个关于PageRank的综述性的调研报告，包含与PageRank相关所有问题，涵盖了*基础的PageRank模型*，*计算方法*，*稳态分布的存在性和唯一性分析*，*收敛速度*，*存储问题*，*基础模型上的改动*，*传统计算方法的改进*等等。\n\n### PageRank的由来\n\n当前的搜索引擎一般通过一个两步的过程来检索与用户查询相关的页面，1）信息检索（Information Retrieval），检索找出所有相关的页面（通常能找出几千个相关的页面 ）；2）**页面排序**（Page Rank），对检索出的页面按照某种准则进行排序，按照顺序展示给用户。PageRank是一个大家熟知的页面排序算法，它基于网页之间的超链接结构，计算出一个页面的重要程度，用于Google搜索引擎。（实际上在Google会综合考虑IR（Information Retrieval） score和PR（PageRank） score决定最终的网页排序，这里我们只关注于PageRank。）\n\nPage, Lawrence和Brin, Sergey于1998年发表了论文，[Page, Lawrence & Brin, Sergey & Motwani, Rajeev & Winograd, Terry. (WWW1998). The PageRank Citation Ranking: Bringing Order to the Web.](http://web.mit.edu/6.033/2004/wwwdocs/papers/page98pagerank.pdf)，第一次提出PageRank算法，并基于此创办了Google公司。\n\n### 基础的PageRank模型\n\n#### 基本概念\n**马尔可夫链（Markov chain）**，考虑状态空间中，从一个状态到另一个状态的转换的随机过程。某一时刻状态转移的概率只依赖于它的前一个状态。这种特定类型的“无记忆性”称作*马尔可夫性质*。满足*马尔可夫性质*的随机过程称为[*马尔可夫链*](https://blog.csdn.net/bitcarmanlee/article/details/82819860)。\n\n**随机性（stochastic）**，矩阵的随机性表示矩阵中的所有元素都是非负的，且每一行的元素和为1。\n\n**不可约性（irreducible）**，不可约的数学定义是“如果从C 中任一状态出发经有限步转移到另一状态的概率都大于0，则称C为不可约闭集”，即任意一种状态都可能转化到任意另外一种状态，即不存在多余的状态（可减少的状态）。\n\n**素矩阵（primitive matrix）**，素矩阵是指自身的某个次幂为正矩阵的矩阵。设$$A$$为一个$$n \\times n$$的方阵，如果存在正整数k使得矩阵$$A^k>0$$那么，称矩阵A为素矩阵。\n\n#### 状态转移矩阵\n\n考虑网页之间的链接关系，可以看成一个有向图，图中节点表示页面，边代表页面之间的超链接关系。一个用户随机浏览网页的过程可以看做是一个在图上节点之间跳转的*随机游走*过程，也是一个马尔可夫链。\n\n考虑各节点之间的转移概率，可以表示成一个矩阵$$P$$，其中$$P_{ij}$$表示通过一步从页面$$i$$跳转到页面$$j$$的概率。例如，假设从一个页面有相等的概率跳转到它所链接的任意页面，则$$P_{ik}=1/d_i, \\forall k \\in N(i)$$（其中$$d_i$$为节点i的出度，$$N(i)$$为节点i的出边邻居集）。也根据自定义的加权方式设置概率分布$$v^T$$，公式表示为$$P'=P+av^T$$，其中$$a$$为一个构造的向量，其中如果i为悬挂节点，则$$a_i=1$$，否则$$a_i=0$$。\n\n由于图中存在*悬挂节点（dangling node）*，即该节点没有出边邻居，此时$$P_i=0^T$$，使得该转移矩阵不满足*随机性*，当随机游走跳转到悬挂节点时，就会停止。为了修正这种情况，我们修改$$P'_i=1/ne^T$$（其中n为图中的总节点数），或者一个自定义的个性化的的随机向量$$P'_i=v^T$$（personalized vector）。\n\n为了保证该马尔可夫链能够收敛得到一个概率分布的稳态向量（stationary vector），也就是最终算得的PageRank值，该马尔可夫链需要满足*不可约性*。所以需要对该状态转移矩阵做进一步的修改，$$P''= \\alpha P'+(1-\\alpha)ev^T/n, 0 \\leq \\alpha \\leq 1$$。\n\n这样通过修正的状态转移矩阵有:\n\n$P''= \\alpha P'+(1-\\alpha)ev^T/n$，\n\n$ \\quad = \\alpha (P+av^T)+(1-\\alpha)ev^T/n$，\n\n$ \\quad = \\alpha P+(\\alpha a+(1-\\alpha)e)v^T$, 其中$0 \\leq \\alpha \\leq 1$，\n\n图中的每个节点都可以通过一步直接到达另一个节点，使得该马尔可夫链满足不可约的性质。该矩阵是一个素矩阵，即通过幂法(power method)迭代计算可以得到一个收敛的稳态向量$$\\pi^T$$。\n\n### 计算PageRank\n\n上述状态转移矩阵代表的马尔可夫链，最终能达到一个稳定状态，即存在一个稳态分布$$\\pi^T$$，使得$$\\pi^TP''=\\pi^T$$，其中$$\\pi^T$$是一个概率向量，$$\\pi^Te=1$$。通过计算该稳态向量$$\\pi^T$$，即得到各个页面的PageRank值，其中$$\\pi^T_i$$即为页面i的PageRank值。\n\n#### 幂法迭代计算\n\n幂法迭代计算是一个传统的计算特征向量问题的方法。\n\n* 设置一个任意的初始向量$$x^{(0)T}$$，通常$$x^{(0)T}=e^T/n$$。\n* 迭代计算：$$x^{(k)T}=x^{(k-1)T}P''$$，即$x^{(k)T}=\\alpha x^{(k-1)T}P+(\\alpha x^{(k-1)T}a+(1-\\alpha))v^T$。\n\n这样，通过一轮又一轮的*向量矩阵相乘*，迭代计算直至达到终止条件，则可计算出稳态向量$$\\pi^T$$。每一轮计算中，需要的$nnz(P)$次浮点运算，$nnz(P)$为P中非零元素的个数。一般P为一个稀疏矩阵，P中每一行的非零元素的个数即为图中每个节点的度数，一般平均为3-10，所以$\\omicron(nnz(P)) \\approx \\omicron(n)$。Brin和Page在论文中指出，一般幂法迭代运算很快就可以达到收敛，通常只需要50-100次迭代运算。\n\n#### 收敛性\n状态转移矩阵$$P''$$的不可约性（irrducibility）保证了该马尔可夫链的唯一一个稳态分布向量的存在性。\n\n而$$P''$$的素性（primitivity）保证了通过幂法迭代计算能够收敛，算出该稳态分布向量。\n\n#### 收敛速度\n\n[矩阵的谱用于分解一个矩阵](https://blog.csdn.net/qq997843911/article/details/88189426)，对于上述的稳态分布$$\\pi^TP''=\\pi^T$$，$$\\pi^T$$就是$$P''$$的一个特征向量，对应的特征值就是1。对$$P''$$所有的特征向量$$v_i^T$$，有$$v_i^TP''=v_i^T c_i$$，其中$$c_i$$为对应的特征值，对$$v_i^T$$的每一轮更新，所有节点的值变为原来的$$c_i$$倍，当$$0 < c_i < 1$$，所有节点值呈指数衰减，直至趋近于0。\n\n我们可以用谱的方法，将上述马尔可夫链的任意初始状态$$x^{(0)T}$$分解成\n\n$x^{(0)T}=v_1^T+c_2 v_2^T+c_3 v_3^T+\\cdots$\n\n对于状态转移矩阵$$P''$$，其最大特征值为1，对应于特征向量$$v_1^T$$，即稳态向量$$\\pi^T$$。其他特征向量$$v_2^T, v_3^T, \\cdots$$对应于特征值$1>c_2>c_3>\\cdots >-1$。则经过t步迭代计算，到达状态$$x^{(t)T}$$\n\n$x^{(t)T}=v_1^T+c_2^t v_2^T+c_3^t v_3^T+\\cdots$\n\n其中$$v_i^T$$部分的分量保持不变，即为我们所求的稳态向量，其他分量随着t增长而指数衰减，最后整个状态$$x^{(t)T}$$收敛趋近于平衡状态。而这个过程的**收敛速度（rate of convergence）**取决于上述非平衡分量中衰减得最慢的那一个，即**第二大特征值$$c_2$$**。$$c_2$$的大小越接近1，收敛越慢，越接近于0，收敛越快。\n\n而$P''$的第二大特征值取决于$\\alpha$。$\\alpha$设置的越小，收敛速度越快，但是网页之间的链接结构对最终计算结果的贡献程度也越小。考虑权衡，Google创始人Brin和Page使用$\\alpha=0.85$。\n\n#### 收敛判断\n\n幂法迭代计算直至满足一个终止条件，一个传统的终止条件是：当k轮计算后的**残差（residual）**$$x^{(k)T}P''-x^{(k)T}=x^{(k+1)T}-x^{(k)T}$$，小于一个预先定义的**容忍度（tolerance ）**$\\tau$时，迭代终止。此时，可以大致估算收敛所需要迭代计算的轮数为$\\frac{log_{10}\\tau}{log_{10}\\alpha}$。当$\\tau=10^{-6}, \\alpha=0.85$时，大致需要 $\\frac{-6}{log_{10}0.85} \\approx 85$ 次迭代计算。上面说到Brin和Page指出，通常只需要50-100次迭代运算，是指$\\tau$取值为$10^{-3}$ 到 $10^{-6}$。\n\n在实际应用中，我们通常只需要计算出页面之间的正确顺序，并不需要知道他们具体的PageRank值。所以我们只需要通过幂法迭代到PageRank向量的排序收敛就可停止。某篇论文在实验展示，在某些数据集上只需10次迭代就能产生良好的近似排序。\n\n#### 加速幂法计算\n\n（1）减少每一轮的计算时间\n\n* 自适应的PageRank，考虑到图中某些节点收敛快，而有些节点收敛慢。所以我们关注于迭代向量中元素，当某些节点已经收敛后，我们就锁定这些节点，不再对他们进行更新计算。\n\n* 图中存在大量悬挂节点，可以划分悬挂节点和非悬挂节点。因为所有悬挂节点对应的行的转移概率是一样的，可以通过一种汇集的聚合方法高效的处理。\n\n（2）减少迭代的次数\n\n* 扩展的Aitken外推法；\n\n* BlockRank；\n\n* 取消对幂法的限制，高斯-赛德尔法，雅可比法。","tags":["random walks","theoretical analysis"]},{"title":"并发图分析系统","url":"/2019/10/16/并发图分析系统/","content":"\n并发图分析任务：大量的图算法并发的运行在同一平台上对底层同一个图数据进行处理，以对该图数据进行多方位的分析处理，获得各种目的性的分析结果。\n\n### 研究工作\n\n[Yu Zhang's Research](https://www.researchgate.net/scientific-contributions/57497079_Yu_Zhang)\n\n#### CGraph\n基于关联性感知的并发图处理\n\n1. [CGraph: A Correlations-aware Approach for Efficient Concurrent Iterative Graph Processing(ATC'18)](https://www.usenix.org/conference/atc18/presentation/zhang-yu)\n2. [CGraph: A Distributed Storage and Processing System for Concurrent Iterative Graph Analysis Jobs(TOS'19)](https://dl.acm.org/citation.cfm?doid=3326597.3319406)\n\n并发图分析任务之间存在关联性。\n\n（1）空间关联性：多个并发图分析任务需要访问和处理的图数据存在大量的交集。\n\n（2）时间关联性：多个并发图分析任务可能需要同时访问同一个图划分块。\n\n但在此前的图处理系统中进行并发图分析任务时，各个任务对共享图数据的访问相互独立，互不感知。使得各个并发的图分析任务在计算时遇到内存墙（内存性能严重限制CPU）和缓存相互干扰的问题。\n\n这篇工作通过感知各个并发图分析任务之间的关联性，使得各个并发图分析任务之间能有效的共享数据和数据访问。\n\n#### DiGraph\n基于关联依赖的单任务图处理\n\n* [DiGraph: An Efficient Path-based Iterative Directed Graph Processing System on Multiple GPUs(ASPLOS'19)](https://dl.acm.org/citation.cfm?doid=3297858.3304029)\n\n* [Efficient Disk-Based Directed Graph Processing: A Strongly Connected Component Approach(TPDS 2018)](https://ieeexplore.ieee.org/document/8118091)\n\n同一个图分析任务的子任务（处理各图划分块的任务）之间也存在依赖关联性。\n\n核心图顶点状态收敛所需要的更新次数决定了整个图收敛所需迭代次数。\n\n这篇工作在单个图分析任务场景下，通过感知图中顶点之间的依赖关联性特征进行图划分，使得各个图划分块之前的依赖关联性尽量符合全序关系，尝试使得未收敛的图划分块对已经收敛的图划分块影响最小化。另外通过加速核心顶点之间的状态传递，进一步加速全局的收敛速度。\n\n#### FBSGraph\n基于路径的异步图处理\n\n* [FBSGraph: Accelerating Asynchronous Graph Processing via Forward and Backward Sweeping(TKDE'18)](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8170287)","tags":["graph processing system","paper reading","concurrent graph processing"]},{"title":"ASAP(OSDI 2018) 分布式图模式挖掘系统","url":"/2019/10/15/ASAP(OSDI-2018)-分布式图模式挖掘系统/","content":"\nASAP: Fast, Approximate Graph Pattern Mining at Scale. Anand Padmanabha Iyer, Zaoxing, Xin Jin, Shivaram Venkataraman, Vladimir Braverman and Ion Stoica, OSDI 2018. [presentation](https://www.usenix.org/conference/osdi18/presentation/iyer) | [paper](https://www.cs.jhu.edu/~xinjin/files/OSDI18_ASAP.pdf)\n<!-- [PPT](https://www.usenix.org/sites/default/files/conference/protected-files/osdi18_slides_iyer.pdf) -->\n\n## 图模式挖掘研究现状\n\n### 图处理算法：\n现有图处理算法可大致分为两类：\n（1）*图分析算法*，e.g. PageRank，社区检测，标签传播。\n（2）*图模式挖掘算法*，e.g. 图形计数（motif counting），频繁子图挖掘（frequent sub-graph mining, FSM），团挖掘（clique mining）。图模式挖掘算法常应用于社交网络中的图元（graphlet）相似性检测，信用卡诈骗侦测等。\n\n现有的图处理系统大多针对图分析算法优化计算，这些系统框架在计算*图分析算法*时，速度很快，并且可以扩展到处理非常大的图数据（e.g. GraM [59] can run one iteration of page rank on a trillionedge graph in 140 seconds in a cluster.）。但是这些系统在计算*图模式挖掘算法*时却很慢，在一个中等大小的图上挖掘简单的模式都需要几个小时。\n\n### 现有图模式挖掘算法\n\n#### 精确计算\n模式挖掘算法中最常用的方法是，从最简单的图模式开始，迭代遍历图中所有可能的*嵌入组合（embeddings）*，检查所有候选的*组合嵌入*，依次修剪掉不能形成最终图模式的*组合嵌入*。\n这个*图模式挖掘算法*的复杂度很大，而且产生的*中间候选集*的大小随着图的规模呈指数增长（1M个节点的图可能就会包含$$10^{17}$$个三角形）。即使在分布式计算中，也需要很大开销来执行**join**来创建和管理这些*中间候选集*。[Arabesque(SOSP 2015)](http://delivery.acm.org/10.1145/2820000/2815410/p425-teixeira.pdf?ip=222.195.68.252&id=2815410&acc=OPENTOC&key=BF85BBA5741FDC6E%2EA4F9C023AC60E700%2E4D4702B0C3E38B35%2E9F04A3A78F7D3B8D&__acm__=1571129497_d7c2b24f90623254ad2b1c0e79d31eb2)针对这个问题，优化了分布式场景下这些*中间候选集*的存储。但是即使经过这些优化，Arabesque依然没有能力处理大规模的图，因为需要*具体化候选集*和*不断在机器间交互候选集数据*。Arabesque在一个由20个256GB内存的机器组成的分布式集群中，统计一个1B个节点的图中的*3节点图模式*，依然需要10个小时。\n\n#### 采样近似计算\n在很多模式挖掘应用中，经常并不需要精确的答案。比如FSM通常只需要输出频繁访问子图出现次数的顺序，图形计数（motif counting）也只需要输出一个指定图形出现的次数。在这些场景下，给出一个近似的答案就已足够。\n\n大数据分析场景下的近似分析已经引起一些关注，这些近似系统的基本思想是：在一小部分抽象的样本数据上执行精确的算法，并根据统计特性进行误差分析。这些系统的一个基础假设是：可以算法的准确性可以由样本的大小决定，并且计算的误差可以推导。\n\n但是，这种假设应用到*图模式挖掘算法*时并不成立。作者通过实验发现，通过减少样本的大小，计算误差和运行效率之间并没有明显的关系，而且即使样本数量设置很大，依然会有很大的计算误差，例如50%的边抽样就会带来大概80%计算误差（详细参见paper中的Figure 1）。\n\n#### 邻居采样\n\n现有的图计算理论中，已经有一些针对特定图模式的近似技术。比如要统计图中三角形的个数，可以从图中随机抽样三条边，看这三条边是否能构成一个三角形，是的话估计值就是$$m^3$$，其中m是图的总边数，否则估计值就是0，通过大量抽样就可以计算平均估计值。这种采样计数理论上确实能达到无偏估计，但是由于m在实际中是很大的，所以随机抽样三条边能构成三角形的概率很小，所以这种采样技术计算的方差非常大，想要达到比较高的近似计算的准确度，需要非常多的抽样估计，从而带来很大的计算和内存开销。\n\n[邻居采样（Neighborhood sampling, VLDB 2013）](http://www.doc88.com/p-3734516936384.html)是最近提出的一种针对*三角形计数（triangle counting）*的近似计算方案。它的基本思想是（边数据以流的形式输入）：\n\n（1）首先从全图随机采样一条边$$l_0$$，采样概率是$$Pr(l_0)=1/m$$；\n\n（2）均匀的从$$l_0$$的邻居边中采样第二条边$$l_1$$，边数据流中，$$l_1$$在$$l_0$$的后面，采样概率是$$Pr(l_1|l_0)=1/c$$，c为边数据流中出现在$$l_0$$后面的$$l_0$$的边邻居数；\n\n（3）在边数据流的$$l_1$$的后面所有边中，找到一条能与$$l_0$$和$$l_1$$构成一个三角形的边$$l_2$$，如果能找到，则这个三角形被采样到的概率是$$Pr(l_0 \\cap l_1 \\cap l_2)=1/mc$$。\n\n上述过程称为一次采样尝试，如果成功采样到一个三角形，则设这次采样计算的图中三角形的个数的估计值为$$e_i=mc$$，否则为0。多次采样，计算平均值（paper中的Figure 2展示了一个五节点完全图的例子）。\n\n#### 依然存在的挑战\n\n1.邻居采样算法只针对一种特定的图模式（三角形），需要扩展到通用的邻居采样方法，使之也能采样其他模式。\n\n2.邻居采样算法是假定图存放在单机的基础上，想要进行大规模的图模式挖掘，需要扩展到分布式图处理。\n\n3.邻居采样算法没有考虑到*属性图*，而现实生活的图模式挖掘通常是针对*属性图*，即节点和边都有类型和属性，因为通常需要*谓词匹配*。\n\n4.需要允许终端用户进行*准确性*和*延迟*之间的权衡。\n\n\n## ASAP中的近似模式挖掘\n\nASAP(A Swift Approximate Pattern-miner)是一个快速、可扩展的分布式近似图模式挖掘系统（paper中的Figure 3展示了ASAP的系统架构）。\n\n### 扩展到通用模式\nASAP推广邻居采样算法到通用的图模式，由两个阶段组成：\n\n（1）采样阶段，在有序的边流中依次采样几条边，分别统计采样概率。\n\n（2）闭合阶段，等待剩下的一条或多条边来构成一个完整的图模式，如果能成功构成，则计算采样到完整图模型的概率，进而计算估计值，否则估计值为0.\n\n对于大于三节点的图模式，有多种采样构成完整图模式的方式，采样概率取决于开发者选择的采样阶段形成的初始模式（paper中的Figure 4展示了采样4-cliques的两种方式）。\n\n#### 通用模式分析\n设$$p*$$为一个$$k$$-节点的图模式，$$p*$$的抽样概率取决于$$k$$和用邻居采样技术采样图模式的不同方式。\n\n* 当$$k=2$$, $$Pr(p=p*,k=2)=1/m$$。\n\n* 当$$k=3$$, $$Pr(p=p*,k=3)=1/m \\cdot c_1$$。\n\n* 当$$k=4$$, $$Pr(p=p*,k=4)=1/m^2$$(type 1)\n$$\\quad \\quad \\quad$$ or $$\\quad Pr(p=p*,k=4)=1/m \\cdot c_1 \\cdot c_2$$(type 2)。\n\n* 当$$k=5$$, $$Pr(p=p*,k=5)=1/m^2 \\cdot c_1$$(type 1)\n$$\\quad \\quad \\quad$$ or $$\\quad Pr(p=p*,k=5)=1/m^2 \\cdot c_2$$(type 2)\n$$\\quad \\quad \\quad$$ or $$\\quad Pr(p=p*,k=5)=1/m \\cdot c_1 \\cdot c_2 \\cdot c_3$$(type 3)\n\n#### 编程接口\n\n* SampleVertex\n\n* SampleEdge\n\n* ConditionalSampleVertex\n\n* ConditionalSampleEdge(subgraph)\n\n* ConditionalClose(subgraph, subgraph)\n\n### 应用到分布式场景\n\nASAP通过2步，将上述图模式挖掘过程扩展到分布式场景：\n\n（1）**并行化采样过程**，由于采样过程中的边数据流没有排序的要求，所以ASAP随机均匀划分节点，使得各个机器上的节点数和边数都尽量均匀。计算时，在多个机器上执行估算任务的多个副本，然后聚合计算结果。\n\n（2）**结合各个机器上的输出结果，修正误差**，由于在分布式集群上计算时，机器之间的边没有办法采样到，所以会导致计算误差。ASAP通过分析误差损失，加权求和纠正误差：$$c=f(w)\\sum^{w-1}_{i=0}{c_i}$$，其中$$w$$为机器个数，$$f(w)$$为纠正权重。以*三角形计数*为例，在全图采样到的所有三角形的三个节点都在同一个机器上的概率是$$1/w^2$$，所以统计三角形个数时，$$f(w)=w^2$$。类似的，统计4-clique时，$$f(w)=w^3$$。\n\n### 属性图中的模式挖掘\n\n#### Predicate Matching\n\n现实生活的图模式挖掘通常是针对*属性图*，因为需要匹配的模式满足一些*谓词*。例如，一个*谓词查询*任务可能会要求统计图中的4-clique，其中clique中每个节点都属于某种特定的类型。ASAP支持两种谓词类型：\n\n（1）**all** ，匹配的模式中*每个*节点和边都满足某个属性。\n（2）**atleast-one**，匹配的模式中*至少有一个*节点和边都满足某个属性。\n\n#### Motif mining\n另一种查询模式是，查找某个特定节点数量的所有模式，称为* motif queries*。\n\n* 3-motif query有2种模式，链式和三角形。\n\n* 4-motif query有6种模式。\n\n其中，有几个模式会有相同的*基础构建块（underlying building block）*。针对这种情况，ASAP节省采样阶段*基础构建块*的构建。\n\n#### 精炼准确度\n有些对图数据的探索性分析场景，需要迭代地完善查询任务。针对这种场景，ASAP保留上一轮的抽样估计结果，在新一轮只需要补充差额。\n\n### 误差延迟配置\n\nASAP提供了两种用户接口，允许用户在准确性和误差之间做权衡，进行误差延迟配置（Error-Latency Profile, ELP）。\n\n* Building Estimator vs. Time Profile。用户指定一个时间预算$$T$$，ASAP返回一个时间$$T$$以内能计算出的最准确的答案，给出误差率保证$$\\epsilon$$和可配置的自信等级（默认95%）。\n\n* Building Estimator vs. Error Profile。用户指定一个误差预算$$\\epsilon$$，ASAP计算出能在最短时间内达到误差范围的答案。\n\n* ASAP也可以实现在动态图场景下快速重建ELP。\n\n\n## 实验\n\n### 实验设置\n\n#### 实现\nASAP部署在Apache Spark上，使用了GraphX中图数据流操作的实现（只使用了简单的map和reduce操作）。ASAP可以实现于任何数据流引擎。\n\n#### 数据集和对比系统\n\n* 数据集：共使用了7个数据集，最大的为UK（106M个节点，3.7B个边）。\n\n* 实验环境：16个Amazon EC2 r4.2xlarge的集群，每个机器有8个虚拟cpu和61GB内存。尽管图能放下一个机器的内存，但是产生的中间状态大大增大了计算的复杂度。\n\n* 使用的模式和指标：3-motifs（2种模式），4-motifs（6种模式），4-cliques。\n\n* 对比系统：Arabesque。\n\n### 对比实验\n\n（1）Overall Performance\n\n* Comparison with Arabesque.\n\n* Scalability on Larger Graphs.\n\n（2）Advanced Pattern Mining\n\n* Motif mining.\n\n* Predicate Matching. \n\n（3）Effectiveness of ELP Techniques\n\n* Time Profile. \n\n* Error Profile. \n\n* Error rate Confidence. \n\n* ELP Building Time. \n\n（4）Scaling ASAP on a Cluster\n\n（5）More Complex Patterns\n\n## 相关工作\n图处理系统\n\n图挖掘系统\n\n近似分析系统\n\n近似图算法\n\n<!-- ## 思考\n思考：分布式图处理系统对网络方面有什么需求?\n\n* 缓存，现有的分布式图计算系统大多数采用*迭代式的计算模型*，一轮计算结束后，机器之间交互信息，然后进行下一轮计算。这一轮传输的信息中可能与上一轮中传输的数据有重合，所以可以考虑缓存？ -->","tags":["graph processing system","paper reading","graph pattern mining"]},{"title":"IMM(SAN) 引入社会活动的影响力最大化问题","url":"/2019/10/10/IMM(SAN)-引入社会活动的影响力最大化问题/","content":"\n[Pengpeng Zhao, Yongkun Li*, Hong Xie, Zhiyong Wu, Yinlong Xu, John C. S. Lui. \"Measuring and Maximizing Influence via Random Walk in Social Activity Networks.\"The 22nd International Conference on Database Systems for Advanced Applications (DASFAA 2017), Suzhou, China, March 2017.](https://link.springer.com/content/pdf/10.1007%2F978-3-319-55699-4_20.pdf).\n\n<!-- ## IMM(SAN) Journal扩展 -- TKDE -->\n\n### 论文大纲\n\n1.**用户活动网络图（SAN），**考虑在线社交网络图（OSN）中用户社交活动的影响，比如对同一个产品的点赞、评论等，基于此提出用户活动网络图（SAN），并提出一种“超图”的概念来表示SAN， 其中一条t型的“超边”连接多个用户，代表这几个用户都参与某种t型活动，比如都对某产品点赞或评高分。\n\n2.**基于RW的影响力中心性，**考虑SAN场景下的影响力最大化问题（IMP(SAN)），采用基于Random Walk的方式来近似估算SAN中大小为k的种子节点集S的影响力，定义为影响力中心性（influence centrality），估算在超图中从每个节点出发进行大量random walks，其中能到达种子节点集S的击中概率（decayed hitting probability）。\n\n3.**贪心算法计算IMP(SAN)，**为计算SAN中的影响力最大化问题（IMP(SAN)），本文采用Monte Carlo框架来估算SAN中的影响力中心性，并提出一种贪心的迭代算法，每一轮选出一个能够带来最大影响力增量的节点，加入到种子节点集S中，具体实现：每一轮，计算每个节点u能够带来的影响力增量，在超图中每个节点出发进行R条L步的random walks，计算这些walks能够访问到$$S\\cup{u}$$的hitting probability。时间复杂度为$$\\omicron(kn^2RL)$$。\n\n4.**算法优化1——并行计算，**每一轮，计算图中每个节点的影响力增量都需要从全图每个节点出发R条L步的random walks，其实这些walk可以共用，即一次性从全图每个节点出发R条L步的random walks，同时计算图中每个节点带来的影响力增量，这样可以将时间复杂度优化到$$\\omicron(knRL)$$。\n\n5.**算法优化2——walk复用，**每一轮都需要从全图每个节点出发R条L步的random walks，同时计算图中每个节点带来的影响力增量，其实第一轮轮的walk信息可以一直被复用到下一轮中，这样可以将时间复杂度优化到$$\\omicron(nRL)$$，这其中会带来walk更新的问题，详细参加paper 5.2。\n\n6.**实验对比，**本文在三个真实世界的数据集上对比了加入用户活动后带来的在独立级联模型（independent cascade model，IC）的影响力传播模型下影响力的增长，以及对比最新的OSN上的影响力最大化问题的最新算法IMM在不同用户活动权重和不同种子节点集大小的配置下效率的改进和能带来的影响力传播。\n\n\n<!-- ### Journal扩展点\n\n1.**从无权图扩展到加权图，** (1)加权图的应用场景，（2）在加权图上的RW需求，（3）加权图上进行一步RW转发的执行过程，（4）时间复杂度分析，（5）利用二分查找优化加权图上的RW过程以及优化后的时间复杂度。实验章节添加从无权图生成加权图的过程。\n\n2.**增加LT模型下的影响力传播,** 6.4 添加对LT模型的介绍以及在LT模型下的实验。\n\n3.**增加一组多种用户和活动类型的实验,**（1）一种用户一种活动，（2）两种用户一种活动，（3）一种用户两种活动。6.4 添加上述三种场景下的实验结果。 -->\n","tags":["random walks","theoretical analysis"]},{"title":"Hexo的配置和使用","url":"/2019/10/10/Hexo的配置和使用/","content":"\n博客搭建，GitHub Page和Hexo的使用\n\nhttps://www.cnblogs.com/ryanleee/p/8274314.html\n\nHexo 和 Markdown 的基本使用规则\n\nhttps://www.jianshu.com/p/56d99a3049a5\n\n在HEXO主题中添加数学公式支持\n\nhttps://www.cnblogs.com/zhyantao/p/10424874.html\n\nMarkDown公式查阅\n\nhttps://blog.csdn.net/EchoWenyu/article/details/95046618\n\nGitment：使用 GitHub Issues 搭建评论系统\n\nhttps://imsun.net/posts/gitment-introduction/\n\n解决gitment无法登录的问题\n\nhttps://cloud.tencent.com/developer/news/316368","tags":["hexo"]},{"title":"Hexo部署错误","url":"/2019/10/09/Hexo部署错误/","content":"\n## Hexo部署错误\n\n生成和本地测试都能通过\n\n``` bash\n$ hexo g\n$ hexo s\n```\n\n但是部署时出错\n\n``` bash\n$ hexo d\n```\n\n错误提示如下：\n\n>...\n>\n>Connection reset by 13.250.177.223 port 22\n>\n>fatal: Could not read from remote repository. &emsp;\n>\n>Please make sure you have the correct access rights and the repository exists.\n>\n>FATAL Something's wrong. Maybe you can find the solution here: https://hexo.io/docs/troubleshooting.html\n>\n>Error: Spawn failed\n>\n>...\n\n搜索相关信息发现应该是GitHub中SSH没有连接上，测试ssh连接GitHub\n\n``` bash\n$ ssh -T git@github.com\n```\n\n连接不成功：\n\n>Connection reset by 13.229.188.59 port 22\n\n## 尝试过的方法\n\n*新建SSH KEY*\n\n*设置防火墙*\n\n都还是不行。\n\n## 最终解决方案\n\n**校园网网络通端口号切换到1号口**","tags":["hexo","github","ssh"]},{"title":"Hello World","url":"/2019/10/08/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n"},{"title":"远程桌面连接","url":"/2019/09/09/远程桌面连接/","content":"\n目标：在本地电脑A通过远程桌面连接控制远程电脑B。\n\n（1）[开启远程电脑B的远程连接](https://jingyan.baidu.com/article/6f2f55a171c4fdb5b93e6c38.html)，这个经验里的第四步的添加用户可不加。\n\n（2）查看远程电脑B的内部IP地址：打开远程电脑B的windows cmd，**ipconfig**命令查看自己的ip地址，一般是192.168.x.x。\n\n（3）在远程电脑B设置外部端口：打开路由器admin，在里面找“虚拟服务器”或者“端口转发”，添加映射关系，内部端口3389，外部端口设置自己喜欢的，内部ip地址就是第二步查出来的。\n\n（4）查看远程电脑B的外部IP地址：打开网络通，找到找到WAN网ip地址，就是主页上显示的那个，一般是222或104等开头。\n\n（5）[在远程电脑B设置静态IP](https://jingyan.baidu.com/article/7f766dafeabc204101e1d0b8.html)，是指第二步查看到的IP，否则下次重启有可能IP地址就变了。\n\n（6）在本地电脑A进行远程连接：打开远程桌面连接，输入**WAN IP:外部端口**，然后输入用户名密码即可登录。\n"},{"title":"Youth","url":"/2019/08/04/Youth/","content":"\n*源于百词斩的一篇阅读分享*\n\nYouth is not a time of life; it is a state of mind;\n\nit is not a matter of rosy cheeks, red lips, and supple knees;\n\nit is a matter of the will, a quality of the imagination, a vigor of the emotions; \n\nit is the freshness of the deep springs of life. \n\nYouth mean a temperamental predominace of courage over timidiity, of the appetite for adventure over the love of ease.\n\nThis often exsits in a man of sixty more than a boy of twenty.\n\nNobody grows old merely by a number of years, \n\nWe grow old by deserting our ideas.\n\nYears may wrinkle the skin, but to give up the enthusiasm wrinkles the soul.\n\nWorry, fear, self-distrust bows the heart and turns the spirit back to dust.\n\nWhether sixty or sixteen, there is in every human being's heart the lure of wonders, the unfailing child-lke appetite of what's next, and the joy of game of living.\n\nIn the center of your heart and my heart, there is a wrieless station;\n\nso long as it receieves the messages of beauty, hope, courage and power from man and from the Infinite, so long as you are young,\n\nWhen the aerials are down, and your spirit is covered with snows of cynicism and ice of pessimism, then you've grown old, even at twenty. but as long as your aerials are up, to catch the waves of optimism, there is hope you may die young at eighty."},{"title":"matlab程序脱离matlab环境运行","url":"/2019/07/01/matlab程序脱离matlab环境运行/","content":"\n我的需求来源于：\n\n* 想用Graph500的图生成器，生成两个大规模的图数据集。在官方github源码中发现了.m文件，测试发现可在matlab中运行生成图数据集。\n\n* 但是，由于我的PC内存容量有限（8GB），最多生成（28，1）的生成图（4GB），即节点数为2^28，边数比节点数为1的子图。\n\n* 我的需求是分别生成一个（30，32）-- 128GB 和（31，128）-- 1TB的生成图，至少需要能够生成（31，1）的子图，然后拼起来，即32GB，内存需求最少为64GB。\n\n* 所以，可以将kronecker_generator.m转成exe可执行文件直接放到64GB服务器上去执行，免去在服务器上装matlab。\n\n\n### 在有matlab的电脑生成.exe可执行文件\n\n1. 生成kronecker_generator.exe文件\n``` bash\nmcc -m kronecker_generator.m\n```\n\n2. 测试：打开cmd命令行，进入kronecker_generator.exe所在目录，运行\n``` bash\nkronecker_generator.exe\n```\n\n3. 带参数运行\n（1）首先在kronecker_generator.exe中添加代码\n\n***\n    function ij = kronecker_generator (SCALE, edgefactor)\n    if (ischar(SCALE))\n        SCALE = str2num(SCALE);\n    end\n    if (ischar(edgefactor))\n        edgefactor = str2num(edgefactor);\n    end\n***\n（2）cmd中测试运行\n``` bash\nkronecker_generator.exe 10 16\n```\n\n### 在另一台电脑上运行\n\n传输可执行文件到目标电脑后执行\n``` bash\nscp -P 5922 kronecker_generator.exe wang@210.45.114.192:/home/wang/code/\nkronecker_generator.exe 10 16\n```\n\n此方法只能用于另一个Windows系统下脱离MATLAB环境运行，Linux中不能直接运行.exe。\n\n要考虑在Linux环境下脱离MATLAB环境运行，需要尝试其他方法，比如将m文件转化为c/c++语言文件，但是还没尝试过。\n\n### 参考\n[matlab程序脱离matlab环境运行-mcc、mbuild和mex命令详解](http://blog.sciencenet.cn/blog-419879-508169.html)","tags":["matlab"]},{"title":"DrunkardMob的编译运行","url":"/2019/06/19/DrunkardMob的编译运行/","content":"\nDrunkardMob部署于Java版本的GraphChi，因为对Java的使用不是很熟练，简单的Java版本的GraphChi的编译和运行折腾了一个多周。这里记录一下编译运行的过程以及其他遇到的问题\n\n#### 安装jdk和mvn\n\n其实简单的apt-get install就可以了\n``` bash\nsudo apt-get install openjdk-8-jdk\nsudo apt-get install maven\n```\n\n所谓后来编译出现的各种问题，就是这里没有装好，我一开始是按照网上一些配置Java运行环境的博客做的，然后配置JAVA_HOME之类的，编译就报各种错，尝试安装了各种版本，还是会有各种奇特的错误。其实这里通过apt-get安装默认是可以不用配置JAVA_HOME环境的。\n\n\n我是先安装的mvn，会自动安装一个jdk，用 java -version看也确实是安装了Java8，但是并没有安装javac，所以编译的时候会找不到编译的包。所以还是需要另外去安装一个jdk。\n\n#### scala版本冲突问题\n\n“class ... is broken”\n\n这里其实是因为作者的pom.xml文件中指定的Scala版本是2.9.0-1， 而实际编译运行需要的Scala的版本是2.11.2.\n\n所以解决办法是：将相应依赖项中scala-lang的版本号改为 2.11.2\n\n***\n    <dependency>\n        <groupId>org.scala-lang</groupId>\n        <artifactId>scala-library</artifactId>\n        <version>2.11.2</version>\n    </dependency>\n***\n\n我上周在这个问题的时候走了很多弯路，看到网上有博客说这个报错是因为需要安装比较老的jdk版本，所以也尝试安装了jdk7和jdk6，mvn的版本也按照作者给的版本重新安装了，总之瞎折腾了很久。\n\n#### 运行找不到类\n\n这个实际上是个很坑的问题，因为后来版本有改进，应该将命令中graphchi-java-0.2-jar-with-dependencies换成graphchi-java-0.2.2-jar-with-dependencies\n\n到这里，不出意外的话，程序应该是可以正常编译，运行起来了，因为最后在403的ccc服务器上，到这里就运行成功了\n\n#### common-math版本冲突\npom.xml中关于common-math有重复定义，可以注释掉其中一个。\n\n#### jdk版本配置\n\n作者用的是jdk1.6，我这边安装的jdk1.8，这里pom.xml文件中maven-compiler-plugin下的configuration中的*source* 和 *target* 应该改为1.8.\n\n\n#### TEST时找不到surefire\n\n解决办法是追加plugin（version要根据报错信息调整，这里应该是2.12.4）\n***\n    <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-surefire-plugin</artifactId>\n        <version>2.12.4</version>\n        <configuration>\n            <useSystemClassLoader>false</useSystemClassLoader>\n        </configuration>\n    </plugin>\n***\n\n#### 在线依赖项\n\n之后遇到的问题是发现修改后的源码，编译后无效，即不能运行编译后的结果。\n又是折腾了两天，发现是因为添加了在线依赖项的原因。。。\n\n即作者的说明文档中，说是要加入的依赖项：\n***\n    For maven, include the following in <dependencies>:\n    <dependency>\n        <groupId>org.graphchi</groupId>\n        <artifactId>graphchi-java_2.11</artifactId>\n        <version>0.2.2</version>\n    </dependency>\n***\n其实是编译好的graphchi-java的库包，被放在的maven的仓库中，添加依赖项就会直接引用在线的库包，从而覆盖了本地的编译的结果。删除掉这项依赖项，可以正常修改然后编译运行了。\n\n#### 版本冲突\n系统不知道什么时候升级了openjdk，导致与pom.xml中声明的版本冲突！\n查看版本：\n``` bash\njava -version\nmvn -version\n```\n\n解决，卸载重装：\n``` bash\nsudo apt-get autoremove openjdk*  //autoremove会卸载所有相关依赖项，慎重使用！！！！\nsudo apt-get autoremove java*\nsudo apt-get install openjdk-8-jdk\n```\n","tags":["Java"]},{"title":"Wukong 基于RDMA的快速并发RDF查询","url":"/2019/02/25/Wukong-基于RDMA的快速并发RDF查询/","content":"\n[Wukong:Fast and Concurrent RDF Queries with RDMA-based Distributed Graph Exploration, JIAXIN SHI, YOUYANG YAO, RONG CHEN, HAIBO CHEN, OSDI16](https://www.usenix.org/conference/osdi16/technical-sessions/presentation/shi)\n\n[可参考博客](https://blog.csdn.net/qq_21125183/article/details/80670024)\n\n### 背景介绍\n\n#### 在线图查询\nOnline graph query plays a vital role for searching, mining and reasoning linked data.\n\n#### RDF and SPARQL\nRDF :  Resource Description Framework 资源描述框架。用于表示网络上链接的数据，例如知识图谱。\n\nRDF is a graph composed by a set of ⟨Subject, Predicate, Object⟩ triples 由<主谓宾>三元组组成的图。\n\nSPARQL ： 针对RDF的标准查询语言。\n\n### 现有查询方法\n\n1、关系型数据库管理（RDBMS）\n\n三元组存储和三元组联接，联接三张表，查询得出结果。\n\n缺点 ：分布式联接开销大 & 会产生大量的中间结果。\n\n2、图处理系统\n\n图存储和图检索，将RDF数据存储为一个图的形式，一步步检索出满足条件的节点关系，最后联接所有查询结果，得出最终结果。\n\n缺点 ：最后联接部分开销非常大 & 查询节点关系需同步执行，比较慢。\n\n## 基于图的RDF数据模型\n\n1. Graph model and indexes\n\n新增协助索引节点（index vertices）：predicate index & type index\n\n原来的节点（normal vertices）：subjects& objects\n\nindex vertices与normal vertices一起参与划分子图和存储\n\n2. Differentiated Graph Partitioning\n受 PowerLyra[ EuroSys15] 启发\n\n1）高度节点 ： 拆分成多个节点，分别存放于不同机器；\n\n2）低度节点 ： 分部存储到不同机器\n\nwukong：\n\n1）index vertices : 拆分；\n\n2）normal vertices ： 分布；\n\n3）在RDF场景下，高度节点的不均匀，并不会很大的影响负载均衡。\n\n3. RDMA-friendly Predicate-based KV store\n\nkey :  \"vid, p/tid, d\"\n\nvalue : the list of neighboring vertex IDs or predicate/type IDs\n\n减少大量计算和网络开销\n\n## wukong查询过程\n\n1. Basic Query Processing\n\n2. Full-history Pruning\n\n3. Migrating Execution or Data\n\n4. Concurrent Query Processing\n","tags":["graph processing system","RDF query","KV graph storage"]}]